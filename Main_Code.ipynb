{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e6rrsFyi4Bd9"
   },
   "outputs": [],
   "source": [
    "# This is the main jupyter notebook for the coursework of MLIS \n",
    "## Grp 4B\n",
    "## Authors\n",
    "## Alpaslan Erdag , Anshuman Singh , Yixin Fan\n",
    "## Date - 15/01/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-IY5zC537OU"
   },
   "source": [
    "<h1> <center> Abstract </center> </h1>\n",
    "​<em>\n",
    "\n",
    "There are many factors that can influence the type of tumor but to various extent. This code will construct machine learning models to predict whether a tumour is malignantor benign based on the observed characters.\n",
    "\n",
    "The data is obtanined from UCI repository which can be obtained from the [here](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28original%29)\n",
    "</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3boaTk7A4FrC",
    "outputId": "28e07a26-0a3f-4422-b507-f887576fdaca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Request:\n",
      "   This breast cancer databases was obtained from the University of Wisconsin\n",
      "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
      "   when using this database, then please include this information in your\n",
      "   acknowledgements.  Also, please cite one or more of:\n",
      "\n",
      "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
      "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
      "\n",
      "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
      "      pattern separation for medical diagnosis applied to breast cytology\", \n",
      "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
      "      December 1990, pp 9193-9196.\n",
      "\n",
      "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
      "      via linear programming: Theory and application to medical diagnosis\", \n",
      "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
      "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
      "\n",
      "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
      "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
      "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
      "\n",
      "1. Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
      "\n",
      "2. Sources:\n",
      "   -- Dr. WIlliam H. Wolberg (physician)\n",
      "      University of Wisconsin Hospitals\n",
      "      Madison, Wisconsin\n",
      "      USA\n",
      "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
      "      Received by David W. Aha (aha@cs.jhu.edu)\n",
      "   -- Date: 15 July 1992\n",
      "\n",
      "3. Past Usage:\n",
      "\n",
      "   Attributes 2 through 10 have been used to represent instances.\n",
      "   Each instance has one of 2 possible classes: benign or malignant.\n",
      "\n",
      "   1. Wolberg,~W.~H., \\& Mangasarian,~O.~L. (1990). Multisurface method of \n",
      "      pattern separation for medical diagnosis applied to breast cytology. In\n",
      "      {\\it Proceedings of the National Academy of Sciences}, {\\it 87},\n",
      "      9193--9196.\n",
      "      -- Size of data set: only 369 instances (at that point in time)\n",
      "      -- Collected classification results: 1 trial only\n",
      "      -- Two pairs of parallel hyperplanes were found to be consistent with\n",
      "         50% of the data\n",
      "         -- Accuracy on remaining 50% of dataset: 93.5%\n",
      "      -- Three pairs of parallel hyperplanes were found to be consistent with\n",
      "         67% of data\n",
      "         -- Accuracy on remaining 33% of dataset: 95.9%\n",
      "\n",
      "   2. Zhang,~J. (1992). Selecting typical instances in instance-based\n",
      "      learning.  In {\\it Proceedings of the Ninth International Machine\n",
      "      Learning Conference} (pp. 470--479).  Aberdeen, Scotland: Morgan\n",
      "      Kaufmann.\n",
      "      -- Size of data set: only 369 instances (at that point in time)\n",
      "      -- Applied 4 instance-based learning algorithms \n",
      "      -- Collected classification results averaged over 10 trials\n",
      "      -- Best accuracy result: \n",
      "         -- 1-nearest neighbor: 93.7%\n",
      "         -- trained on 200 instances, tested on the other 169\n",
      "      -- Also of interest:\n",
      "         -- Using only typical instances: 92.2% (storing only 23.1 instances)\n",
      "         -- trained on 200 instances, tested on the other 169\n",
      "\n",
      "4. Relevant Information:\n",
      "\n",
      "   Samples arrive periodically as Dr. Wolberg reports his clinical cases.\n",
      "   The database therefore reflects this chronological grouping of the data.\n",
      "   This grouping information appears immediately below, having been removed\n",
      "   from the data itself:\n",
      "\n",
      "     Group 1: 367 instances (January 1989)\n",
      "     Group 2:  70 instances (October 1989)\n",
      "     Group 3:  31 instances (February 1990)\n",
      "     Group 4:  17 instances (April 1990)\n",
      "     Group 5:  48 instances (August 1990)\n",
      "     Group 6:  49 instances (Updated January 1991)\n",
      "     Group 7:  31 instances (June 1991)\n",
      "     Group 8:  86 instances (November 1991)\n",
      "     -----------------------------------------\n",
      "     Total:   699 points (as of the donated datbase on 15 July 1992)\n",
      "\n",
      "   Note that the results summarized above in Past Usage refer to a dataset\n",
      "   of size 369, while Group 1 has only 367 instances.  This is because it\n",
      "   originally contained 369 instances; 2 were removed.  The following\n",
      "   statements summarizes changes to the original Group 1's set of data:\n",
      "\n",
      "   #####  Group 1 : 367 points: 200B 167M (January 1989)\n",
      "   #####  Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805\n",
      "   #####  Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record\n",
      "   #####                  : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial\n",
      "   #####                  : Changed 0 to 1 in field 6 of sample 1219406\n",
      "   #####                  : Changed 0 to 1 in field 8 of following sample:\n",
      "   #####                  : 1182404,2,3,1,1,1,2,0,1,1,1\n",
      "\n",
      "5. Number of Instances: 699 (as of 15 July 1992)\n",
      "\n",
      "6. Number of Attributes: 10 plus the class attribute\n",
      "\n",
      "7. Attribute Information: (class attribute has been moved to last column)\n",
      "\n",
      "   #  Attribute                     Domain\n",
      "   -- -----------------------------------------\n",
      "   1. Sample code number            id number\n",
      "   2. Clump Thickness               1 - 10\n",
      "   3. Uniformity of Cell Size       1 - 10\n",
      "   4. Uniformity of Cell Shape      1 - 10\n",
      "   5. Marginal Adhesion             1 - 10\n",
      "   6. Single Epithelial Cell Size   1 - 10\n",
      "   7. Bare Nuclei                   1 - 10\n",
      "   8. Bland Chromatin               1 - 10\n",
      "   9. Normal Nucleoli               1 - 10\n",
      "  10. Mitoses                       1 - 10\n",
      "  11. Class:                        (2 for benign, 4 for malignant)\n",
      "\n",
      "8. Missing attribute values: 16\n",
      "\n",
      "   There are 16 instances in Groups 1 to 6 that contain a single missing \n",
      "   (i.e., unavailable) attribute value, now denoted by \"?\".  \n",
      "\n",
      "9. Class distribution:\n",
      " \n",
      "   Benign: 458 (65.5%)\n",
      "   Malignant: 241 (34.5%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the Data\n",
    "about_data ='/Users/anshuman/Desktop/Project_Folder/MLIS_Project_Ideal/breast-cancer-wisconsin.names'\n",
    "#about_data = '/content/breast-cancer-wisconsin.names'\n",
    "with open(about_data) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN6gh3N24R-W"
   },
   "source": [
    "<h1><center>Code Division</center></h1>\n",
    "\n",
    "<h2><em>The code is divided in the following parts</em></h2>\n",
    "<ol>\n",
    "<li> <h4> Data Cleaning and Data Preprocessing </h4></li> \n",
    "<li> <h4>Model Fiting on the clean data </h4></li> \n",
    "<li> <h4>Model Accuracy </h4></li>\n",
    "</ol>\n",
    "</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TONMQnQ4SDg"
   },
   "source": [
    "<h2><center>1. Data Cleaning and Data Preprocessing</center></h2>\n",
    "\n",
    "<p>To get a good result over model accuracies we are going to first clean the data and then used the cleaned and preprocessed data to train a model. </p>\n",
    "\n",
    "<p> We will be using a systematic approach to clean our data. That will be listed in the following steps</p>\n",
    "<em>\n",
    "&emsp; <li>Data Visulaization</li>\n",
    "&emsp; <li>Checking and Removing nan values</li>\n",
    "&emsp; <li>Removing Useless Attributes</li>\n",
    "&emsp; <li>Deleting Outliers and Data Normalizations</li>\n",
    "</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-daRoybV4SJI"
   },
   "source": [
    "<h3>1.1 Data Visualization </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRQhoFvkfl-R"
   },
   "source": [
    "<h4>1.1.1 Code </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sK9S6Bky4RlC"
   },
   "outputs": [],
   "source": [
    "## Header Files \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import scipy\n",
    "\n",
    "## Loading Data \n",
    "data = '/Users/anshuman/Desktop/Project_Folder/MLIS_Project_Ideal/breast-cancer-wisconsin.data'\n",
    "#data = '/content/breast-cancer-wisconsin.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xL5c7NTs4omD",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Samplecodenumber</th>\n",
       "      <th>ClumpThickness</th>\n",
       "      <th>UniformityofCellSize</th>\n",
       "      <th>UniformityofCellShape</th>\n",
       "      <th>MarginalAdhesion</th>\n",
       "      <th>SingleEpithelialCellSize</th>\n",
       "      <th>BareNuclei</th>\n",
       "      <th>BlandChromatin</th>\n",
       "      <th>NormalNucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Samplecodenumber  ClumpThickness  UniformityofCellSize  \\\n",
       "0           1000025               5                     1   \n",
       "1           1002945               5                     4   \n",
       "2           1015425               3                     1   \n",
       "3           1016277               6                     8   \n",
       "4           1017023               4                     1   \n",
       "\n",
       "   UniformityofCellShape  MarginalAdhesion  SingleEpithelialCellSize  \\\n",
       "0                      1                 1                         2   \n",
       "1                      4                 5                         7   \n",
       "2                      1                 1                         2   \n",
       "3                      8                 1                         3   \n",
       "4                      1                 3                         2   \n",
       "\n",
       "  BareNuclei  BlandChromatin  NormalNucleoli  Mitoses  Class  \n",
       "0          1               3               1        1      2  \n",
       "1         10               3               2        1      2  \n",
       "2          2               3               1        1      2  \n",
       "3          4               3               7        1      2  \n",
       "4          1               3               1        1      2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nameing the columms\n",
    "col_name = ['Samplecodenumber','ClumpThickness','UniformityofCellSize','UniformityofCellShape',\n",
    "            'MarginalAdhesion','SingleEpithelialCellSize','BareNuclei',\n",
    "            'BlandChromatin','NormalNucleoli','Mitoses','Class']\n",
    "cancerdata = pd.read_csv(data,low_memory=False,names=col_name)\n",
    "cancerdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oOGBeeDS4opZ"
   },
   "outputs": [],
   "source": [
    "## Removing attribute that is of no use\n",
    "cancerdata = cancerdata.drop(['Samplecodenumber'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nV7-qayy4orv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    458\n",
       "4    241\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of samples present by class count\n",
    "cancerdata['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iY_4Kw5F4ous"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClumpThickness</th>\n",
       "      <th>UniformityofCellSize</th>\n",
       "      <th>UniformityofCellShape</th>\n",
       "      <th>MarginalAdhesion</th>\n",
       "      <th>SingleEpithelialCellSize</th>\n",
       "      <th>BareNuclei</th>\n",
       "      <th>BlandChromatin</th>\n",
       "      <th>NormalNucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ClumpThickness  UniformityofCellSize  UniformityofCellShape  \\\n",
       "0               5                     4                      4   \n",
       "1               4                     1                      1   \n",
       "2               8                    10                     10   \n",
       "3               1                     1                      1   \n",
       "4               2                     1                      2   \n",
       "\n",
       "   MarginalAdhesion  SingleEpithelialCellSize BareNuclei  BlandChromatin  \\\n",
       "0                 5                         7         10               3   \n",
       "1                 3                         2          1               3   \n",
       "2                 8                         7         10               9   \n",
       "3                 1                         2         10               3   \n",
       "4                 1                         2          1               3   \n",
       "\n",
       "   NormalNucleoli  Mitoses  classes  \n",
       "0               2        1        0  \n",
       "1               1        1        0  \n",
       "2               7        1        1  \n",
       "3               1        1        0  \n",
       "4               1        1        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the class counts to similar size\n",
    "Benign = cancerdata[(cancerdata.Class == 2) ].sample(240).index\n",
    "Malignant = cancerdata[(cancerdata.Class == 4) ].sample(240).index\n",
    "cancer = cancerdata.loc[Benign|Malignant]\n",
    "cancer = cancer.reset_index(drop=True)\n",
    "\n",
    "# Making the classess from 2/4 to 0/1 \n",
    "cancer['classes'] = cancer.Class.map({2:0,4:1})\n",
    "cancer = cancer.drop(['Class'], 1)\n",
    "\n",
    "# Printing the updated dataframe\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2gLfIN6F4oxH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    240\n",
       "1    240\n",
       "Name: classes, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming that counts of the classes are similar \n",
    "cancer['classes'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FxwBcGnk429U",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot date variables and find the variables with similar distributions, keep only one of them.\n",
    "#['ClumpThickness','UniformityofCellSize','UniformityofCellShape','MarginalAdhesion',\n",
    "#'SingleEpithelialCellSize','BareNuclei','BlandChromatin','NormalNucleoli','Mitoses']\n",
    "fig, axs = plt.subplots(3, 3, figsize=(18,18))\n",
    "axs[0,0].hist(cancer['ClumpThickness'])\n",
    "axs[0,0].set_title(\"ClumpThickness\")\n",
    "axs[0,1].hist(cancer['UniformityofCellSize'])\n",
    "axs[0,1].set_title(\"UniformityofCellSize\")\n",
    "axs[0,2].hist(cancer['UniformityofCellShape'])\n",
    "axs[0,2].set_title(\"UniformityofCellShape\")\n",
    "axs[1,0].hist(cancer['MarginalAdhesion'])\n",
    "axs[1,0].set_title(\"MarginalAdhesion\")\n",
    "axs[1,1].hist(cancer['SingleEpithelialCellSize'])\n",
    "axs[1,1].set_title(\"SingleEpithelialCellSize\")\n",
    "axs[1,2].hist(cancer['BareNuclei'])\n",
    "axs[1,2].set_title(\"BareNuclei\")\n",
    "axs[2,0].hist(cancer['BlandChromatin'])\n",
    "axs[2,0].set_title(\"BlandChromatin\")\n",
    "axs[2,1].hist(cancer['NormalNucleoli'])\n",
    "axs[2,1].set_title(\"NormalNucleoli\")\n",
    "axs[2,2].hist(cancer['Mitoses'])\n",
    "axs[2,2].set_title(\"Mitoses\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdpWrB3HWel7"
   },
   "source": [
    "<h4>1.1.2 Observations </h4>\n",
    "<ol>\n",
    "    <li>The class counts is in a 2:1 ratio with benign being 458 and malignant being 241</li>\n",
    "    <li>The graph shows that data is very skewed for every attribute</li>\n",
    "</ol>\n",
    "<h4>1.1.3 Approach </h4>\n",
    "<ol>\n",
    "    <li>First we make the class ratio 1:1 and making data points to 240 for each class</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPcTVTlE46vC"
   },
   "source": [
    "<h3>1.2 Checking and Removing nan values</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbaWQwlpfyif"
   },
   "source": [
    "<h4>1.2.1 Code </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dJclAfmh49KE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int64     9\n",
       "object    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing all the value counts \n",
    "cancer.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3DCa8TIj4_Ty"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BareNuclei</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    BareNuclei\n",
       "0           10\n",
       "1            1\n",
       "2           10\n",
       "3           10\n",
       "4            1\n",
       "..         ...\n",
       "475          1\n",
       "476          5\n",
       "477          3\n",
       "478          4\n",
       "479          5\n",
       "\n",
       "[480 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing data types\n",
    "cancer.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1CyRzBnb6XnU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BareNuclei</th>\n",
       "      <td>10</td>\n",
       "      <td>2.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Count   Percent\n",
       "BareNuclei     10  2.083333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replacing nan Values\n",
    "cancer = cancer.replace('?' ,np.nan)\n",
    "\n",
    "##Finding the the count and percentage of values that are missing in the dataframe.\n",
    "null1 = pd.DataFrame({'Count': cancer.isnull().sum(), 'Percent': 100*cancer.isnull().sum()/len(cancer)})\n",
    "\n",
    "##printing columns with null count more than 0\n",
    "null1[null1['Count'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mQKh6EdQ6Xrk"
   },
   "outputs": [],
   "source": [
    "## Filling the columns with nan values with mean of the data\n",
    "cancer[\"BareNuclei\"]=cancer[\"BareNuclei\"].astype(float)\n",
    "cancer=cancer.fillna(cancer.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHIdqxDmWMPe"
   },
   "source": [
    "<h4>1.2.2 Observations </h4>\n",
    "<ol>\n",
    "    <li>We obsereve that there is one attribute that is not in integer format</li>\n",
    "    <li>We find that there are nan values in BareNuclei. It also mentioned in the .names file </li>\n",
    "</ol>\n",
    "<h4>1.2.3 Approach </h4>\n",
    "<ol>\n",
    "    <li>We find that BareNuclei is a object type attribute so we convert it to float</li>\n",
    "    <li>We replace nan values in BareNuclei with mean value of BareNuclei</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf89AePf6nsZ"
   },
   "source": [
    "<h3>1.3 Removing Useless Attributes </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aF8wcn9Gf3au"
   },
   "source": [
    "<h4>1.3.1 Code </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "e1xo7hxe6Xv1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classes</th>\n",
       "      <th>BareNuclei</th>\n",
       "      <th>UniformityofCellShape</th>\n",
       "      <th>UniformityofCellSize</th>\n",
       "      <th>BlandChromatin</th>\n",
       "      <th>ClumpThickness</th>\n",
       "      <th>NormalNucleoli</th>\n",
       "      <th>MarginalAdhesion</th>\n",
       "      <th>SingleEpithelialCellSize</th>\n",
       "      <th>Mitoses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classes  BareNuclei  UniformityofCellShape  UniformityofCellSize  \\\n",
       "0        0        10.0                      4                     4   \n",
       "1        0         1.0                      1                     1   \n",
       "2        1        10.0                     10                    10   \n",
       "\n",
       "   BlandChromatin  ClumpThickness  NormalNucleoli  MarginalAdhesion  \\\n",
       "0               3               5               2                 5   \n",
       "1               3               4               1                 3   \n",
       "2               9               8               7                 8   \n",
       "\n",
       "   SingleEpithelialCellSize  Mitoses  \n",
       "0                         7        1  \n",
       "1                         2        1  \n",
       "2                         7        1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorting with correlation values\n",
    "corr_list1=cancer.corr()['classes'].abs().sort_values(ascending=False)\n",
    "corr_list_new=corr_list1[corr_list1>0.01].index.values.tolist()\n",
    "corr_list1\n",
    "cancer=cancer[corr_list_new]\n",
    "cancer.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5MgSXzzR6XzR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classes                   classes                     1.000000\n",
       "UniformityofCellShape     UniformityofCellSize        0.896135\n",
       "BareNuclei                classes                     0.790426\n",
       "classes                   UniformityofCellShape       0.790049\n",
       "UniformityofCellSize      classes                     0.789792\n",
       "BlandChromatin            UniformityofCellSize        0.736743\n",
       "classes                   BlandChromatin              0.732656\n",
       "SingleEpithelialCellSize  UniformityofCellSize        0.723037\n",
       "BlandChromatin            UniformityofCellShape       0.716162\n",
       "classes                   ClumpThickness              0.712987\n",
       "SingleEpithelialCellSize  UniformityofCellShape       0.686486\n",
       "UniformityofCellShape     NormalNucleoli              0.684205\n",
       "UniformityofCellSize      NormalNucleoli              0.682809\n",
       "NormalNucleoli            classes                     0.677401\n",
       "UniformityofCellSize      MarginalAdhesion            0.667173\n",
       "UniformityofCellShape     BareNuclei                  0.664897\n",
       "classes                   MarginalAdhesion            0.649443\n",
       "UniformityofCellShape     MarginalAdhesion            0.641083\n",
       "BareNuclei                UniformityofCellSize        0.635818\n",
       "SingleEpithelialCellSize  classes                     0.635325\n",
       "MarginalAdhesion          BlandChromatin              0.634445\n",
       "BlandChromatin            BareNuclei                  0.633738\n",
       "ClumpThickness            UniformityofCellShape       0.629534\n",
       "BlandChromatin            NormalNucleoli              0.627880\n",
       "ClumpThickness            UniformityofCellSize        0.617340\n",
       "BareNuclei                MarginalAdhesion            0.614693\n",
       "SingleEpithelialCellSize  NormalNucleoli              0.585236\n",
       "                          BlandChromatin              0.572572\n",
       "BareNuclei                ClumpThickness              0.557192\n",
       "NormalNucleoli            MarginalAdhesion            0.551852\n",
       "SingleEpithelialCellSize  MarginalAdhesion            0.549297\n",
       "BlandChromatin            ClumpThickness              0.537194\n",
       "SingleEpithelialCellSize  BareNuclei                  0.519750\n",
       "BareNuclei                NormalNucleoli              0.513929\n",
       "ClumpThickness            NormalNucleoli              0.495772\n",
       "SingleEpithelialCellSize  ClumpThickness              0.478527\n",
       "Mitoses                   SingleEpithelialCellSize    0.457793\n",
       "MarginalAdhesion          ClumpThickness              0.436892\n",
       "Mitoses                   UniformityofCellSize        0.430463\n",
       "                          UniformityofCellShape       0.409997\n",
       "NormalNucleoli            Mitoses                     0.401391\n",
       "MarginalAdhesion          Mitoses                     0.387373\n",
       "classes                   Mitoses                     0.385516\n",
       "ClumpThickness            Mitoses                     0.329697\n",
       "Mitoses                   BlandChromatin              0.311826\n",
       "BareNuclei                Mitoses                     0.291226\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## To remove outliers first we have to find the correlation between attributes\n",
    "\n",
    "#find the variables with high correlations\n",
    "cor1 = cancer.corr().abs()\n",
    "list1 = cor1.stack().sort_values(ascending=False).drop_duplicates()  \n",
    "high_corr= list1[list1>0.70].index.values.tolist()\n",
    "high_corr.remove(high_corr[0])\n",
    "\n",
    "display(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "x7OKFrzE6X3D"
   },
   "outputs": [],
   "source": [
    "# for variable pairs with high correlation, keep only one of them\n",
    "columnlist=list(cancer.columns)\n",
    "len(high_corr)\n",
    "for i in range(len(high_corr)):\n",
    "    if \"classes\" in high_corr[i]:\n",
    "        columnlist=columnlist\n",
    "    else:\n",
    "        if high_corr[i][0] in columnlist and high_corr[i][1] in columnlist:\n",
    "            columnlist.remove(high_corr[i][1])\n",
    "        else:\n",
    "            columnlist=columnlist\n",
    "cancer_final=cancer[columnlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZJPMB2Tq6pOq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 360 120 120\n"
     ]
    }
   ],
   "source": [
    "# Numpy Reference in [1]\n",
    "# To shuffle data and then split  \n",
    "def shuffle_split_data(x, y):\n",
    "    arr_rand = np.random.rand(x.shape[0])\n",
    "    split = arr_rand < np.percentile(arr_rand, 75)\n",
    "\n",
    "    x_train = x[split]\n",
    "    y_train = y[split]\n",
    "    x_test =  x[~split]\n",
    "    y_test = y[~split]\n",
    "\n",
    "    print(len(x_train), len(y_train), len(x_test), len(y_test))\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "# Dividing the data in classes  \n",
    "x = cancer_final.drop(['classes'], 1)\n",
    "y = cancer_final[['classes']]\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "train_x_norm,train_y_norm,test_x_norm,test_y_norm = shuffle_split_data(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-oNBnzbV4lI"
   },
   "source": [
    "<h4>1.3.2 Observations </h4>\n",
    "<ol>\n",
    "    <li>We find the correlation list of the attributes</li>\n",
    "</ol>\n",
    "\n",
    "<h4>1.3.3 Approach </h4>\n",
    "<ol>\n",
    "    <li>We remove the attributes that have low correlations from the data frame</li>\n",
    "    <li>We split the data in test and train datasets</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mQKJvN27Bfu"
   },
   "source": [
    "<h3>1.4 Deleting Outliers and Data Normalizations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOcvGwDUf8fd"
   },
   "source": [
    "<h4>1.4.1 Code </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9Z4bemp56pTY"
   },
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "cancer_norm = cancer_final[(np.abs(scipy.stats.zscore(cancer_final)) < 3).all(axis=1)]\n",
    "# Zero mean normalisation\n",
    "cancer_norm.iloc[:,1:]=(cancer_norm.iloc[:,1:]-cancer_norm.iloc[:,1:].mean())/cancer_norm.iloc[:,1:].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MQpTYu396pXt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classes</th>\n",
       "      <th>BareNuclei</th>\n",
       "      <th>BlandChromatin</th>\n",
       "      <th>ClumpThickness</th>\n",
       "      <th>NormalNucleoli</th>\n",
       "      <th>MarginalAdhesion</th>\n",
       "      <th>SingleEpithelialCellSize</th>\n",
       "      <th>Mitoses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.435322</td>\n",
       "      <td>-0.365591</td>\n",
       "      <td>0.020646</td>\n",
       "      <td>-0.424254</td>\n",
       "      <td>0.542549</td>\n",
       "      <td>1.476816</td>\n",
       "      <td>-0.405173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.872741</td>\n",
       "      <td>-0.365591</td>\n",
       "      <td>-0.317805</td>\n",
       "      <td>-0.727576</td>\n",
       "      <td>-0.108510</td>\n",
       "      <td>-0.672387</td>\n",
       "      <td>-0.405173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.435322</td>\n",
       "      <td>1.965049</td>\n",
       "      <td>1.035999</td>\n",
       "      <td>1.092355</td>\n",
       "      <td>1.519138</td>\n",
       "      <td>1.476816</td>\n",
       "      <td>-0.405173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.435322</td>\n",
       "      <td>-0.365591</td>\n",
       "      <td>-1.333157</td>\n",
       "      <td>-0.727576</td>\n",
       "      <td>-0.759569</td>\n",
       "      <td>-0.672387</td>\n",
       "      <td>-0.405173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.872741</td>\n",
       "      <td>-0.365591</td>\n",
       "      <td>-0.994706</td>\n",
       "      <td>-0.727576</td>\n",
       "      <td>-0.759569</td>\n",
       "      <td>-0.672387</td>\n",
       "      <td>-0.405173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classes  BareNuclei  BlandChromatin  ClumpThickness  NormalNucleoli  \\\n",
       "0        0    1.435322       -0.365591        0.020646       -0.424254   \n",
       "1        0   -0.872741       -0.365591       -0.317805       -0.727576   \n",
       "2        1    1.435322        1.965049        1.035999        1.092355   \n",
       "3        0    1.435322       -0.365591       -1.333157       -0.727576   \n",
       "4        0   -0.872741       -0.365591       -0.994706       -0.727576   \n",
       "\n",
       "   MarginalAdhesion  SingleEpithelialCellSize   Mitoses  \n",
       "0          0.542549                  1.476816 -0.405173  \n",
       "1         -0.108510                 -0.672387 -0.405173  \n",
       "2          1.519138                  1.476816 -0.405173  \n",
       "3         -0.759569                 -0.672387 -0.405173  \n",
       "4         -0.759569                 -0.672387 -0.405173  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the normalized head of the data\n",
    "cancer_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Reference in [1]\n",
    "\n",
    "# To shuffle the data and split data\n",
    "def shuffle_split_data(x, y):\n",
    "    \n",
    "    # To randomize \n",
    "    arr_rand = np.random.rand(x.shape[0])\n",
    "    \n",
    "    # To split numpy percentile by frequency \n",
    "    split = arr_rand < np.percentile(arr_rand, 75)\n",
    "\n",
    "    x_train = x[split]\n",
    "    y_train = y[split]\n",
    "    x_test =  x[~split]\n",
    "    y_test = y[~split]\n",
    "    \n",
    "    # Return train and test\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# Dividing the data in classes \n",
    "x = cancer_norm.drop(['classes'], 1)\n",
    "y = cancer_norm[['classes']]\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "train_x_norm,train_y_norm,test_x_norm,test_y_norm = shuffle_split_data(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlkLRQkM6pee",
    "outputId": "8fc8fc33-4eb4-440d-dd77-37a6680cef4c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape : (344, 7)\n",
      "Y_train Shape : (344, 1)\n",
      "X_test Shape  : (115, 7)\n",
      "Y_test Shape  : (115, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the data attributes\n",
    "x_train = np.array(train_x_norm)\n",
    "print('X_train Shape :',x_train.shape)\n",
    "y_train = np.array(train_y_norm)\n",
    "print('Y_train Shape :',y_train.shape)\n",
    "x_test = np.array(test_x_norm)\n",
    "print('X_test Shape  :',x_test.shape)\n",
    "y_test = np.array(test_y_norm)\n",
    "print('Y_test Shape  :',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(344,)\n",
      "(115,)\n"
     ]
    }
   ],
   "source": [
    "# Changed the shape of the y_train from (x,1) to (x,)\n",
    "y_train=y_train.reshape(len(y_train))\n",
    "print(y_train.shape)\n",
    "y_test=y_test.reshape(len(y_test))\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezxSeKxKWqBF"
   },
   "source": [
    "<h3>1.5 Conclusion </h3>\n",
    "<ol>\n",
    "    <li>We removed the NAN values from our data and replaced them with mean values</li>\n",
    "    <li>We only keep highly correlated attributes in our final attribute list</li>\n",
    "    <li>We filter our data using the zscores and then normalize it usind mean and standard deviation</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNNGSB4Vb2CE"
   },
   "source": [
    "<h2><center>2. Model Fiting on the clean data</center></h2>\n",
    "\n",
    "We have a cleaned and a normalized data now we just have to fit a model over the data and make predictions.\n",
    "We are going to implement 2 models that are implemented by us \n",
    "\n",
    "<ol>\n",
    "    <li>SGD Classifier with Log Loss</li>\n",
    "    <li>SVM with RBF kernel</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGeB6fiFYPuS"
   },
   "source": [
    "<h3>2.1 SGD Classifier with Log Loss</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NIP2zzzYi2e"
   },
   "source": [
    "<h4>2.1.1 Function Call</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qq2Z7Etj6yun"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 18/300 [00:00<00:01, 173.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Epoch no(iteration no)  1\n",
      "W intercept: [0.00589366 0.0071683  0.00944719 0.00659084 0.00534027 0.00666425\n",
      " 0.00314309], B intercept: 0.0014940778113680297, Train loss: 0.29451, Test loss: 0.29490\n",
      "\n",
      "-- Epoch no(iteration no)  2\n",
      "W intercept: [0.0135065  0.00944206 0.01828957 0.01382438 0.00864489 0.01572926\n",
      " 0.00984201], B intercept: 0.0029891084518740116, Train loss: 0.28823, Test loss: 0.28914\n",
      "\n",
      "-- Epoch no(iteration no)  3\n",
      "W intercept: [0.02374891 0.01449139 0.02527925 0.01637294 0.01601309 0.02351505\n",
      " 0.01607061], B intercept: 0.0053539747967795105, Train loss: 0.28179, Test loss: 0.28328\n",
      "\n",
      "-- Epoch no(iteration no)  4\n",
      "W intercept: [0.03483868 0.02265907 0.03244267 0.02779184 0.02636277 0.03460539\n",
      " 0.02175927], B intercept: 0.004686592618930843, Train loss: 0.27289, Test loss: 0.27493\n",
      "\n",
      "-- Epoch no(iteration no)  5\n",
      "W intercept: [0.04388875 0.03178201 0.0433448  0.03903144 0.03378178 0.04202868\n",
      " 0.02526803], B intercept: 0.009463111259034655, Train loss: 0.26500, Test loss: 0.26745\n",
      "\n",
      "-- Epoch no(iteration no)  6\n",
      "W intercept: [0.0564442  0.03715025 0.05154394 0.04272869 0.03987323 0.04785916\n",
      " 0.02862128], B intercept: 0.011756623608197616, Train loss: 0.25896, Test loss: 0.26180\n",
      "\n",
      "-- Epoch no(iteration no)  7\n",
      "W intercept: [0.06483545 0.04486131 0.06222524 0.05263939 0.04730042 0.05442069\n",
      " 0.03328304], B intercept: 0.01475476647434803, Train loss: 0.25194, Test loss: 0.25516\n",
      "\n",
      "-- Epoch no(iteration no)  8\n",
      "W intercept: [0.0748217  0.05036129 0.06989928 0.05846694 0.05603502 0.0584979\n",
      " 0.04041442], B intercept: 0.015163795115457252, Train loss: 0.24599, Test loss: 0.24960\n",
      "\n",
      "-- Epoch no(iteration no)  9\n",
      "W intercept: [0.08404871 0.05873751 0.08032849 0.06496806 0.06144182 0.06689524\n",
      " 0.0420579 ], B intercept: 0.011105658513092479, Train loss: 0.23972, Test loss: 0.24354\n",
      "\n",
      "-- Epoch no(iteration no)  10\n",
      "W intercept: [0.09197535 0.06852852 0.08591156 0.07063177 0.06838168 0.07283628\n",
      " 0.04537611], B intercept: 0.009068276539845642, Train loss: 0.23431, Test loss: 0.23837\n",
      "\n",
      "-- Epoch no(iteration no)  11\n",
      "W intercept: [0.1003619  0.07724026 0.09046906 0.07795931 0.07614181 0.07926252\n",
      " 0.04998237], B intercept: 0.0034660184910708424, Train loss: 0.22880, Test loss: 0.23306\n",
      "\n",
      "-- Epoch no(iteration no)  12\n",
      "W intercept: [0.10920304 0.08803264 0.09566407 0.08404757 0.08217617 0.08534737\n",
      " 0.05177971], B intercept: 0.002945484026602221, Train loss: 0.22359, Test loss: 0.22805\n",
      "\n",
      "-- Epoch no(iteration no)  13\n",
      "W intercept: [0.11609427 0.09821733 0.10197011 0.08986261 0.09015893 0.09159703\n",
      " 0.05571625], B intercept: -0.0015505618817702959, Train loss: 0.21838, Test loss: 0.22301\n",
      "\n",
      "-- Epoch no(iteration no)  14\n",
      "W intercept: [0.12090435 0.10792346 0.10712338 0.09804795 0.09717784 0.09696953\n",
      " 0.05827406], B intercept: -0.005389245218297456, Train loss: 0.21378, Test loss: 0.21851\n",
      "\n",
      "-- Epoch no(iteration no)  15\n",
      "W intercept: [0.12527852 0.11201802 0.114965   0.10287379 0.09940568 0.10179504\n",
      " 0.06318514], B intercept: 0.0003497606246664163, Train loss: 0.21044, Test loss: 0.21543\n",
      "\n",
      "-- Epoch no(iteration no)  16\n",
      "W intercept: [0.13175115 0.11320201 0.12126891 0.10724843 0.10283149 0.10737073\n",
      " 0.06778455], B intercept: 0.0012375801724412128, Train loss: 0.20723, Test loss: 0.21245\n",
      "\n",
      "-- Epoch no(iteration no)  17\n",
      "W intercept: [0.14065032 0.11740042 0.1274197  0.10989124 0.10832043 0.11424638\n",
      " 0.07136125], B intercept: 0.0013427432730583918, Train loss: 0.20337, Test loss: 0.20883\n",
      "\n",
      "-- Epoch no(iteration no)  18\n",
      "W intercept: [0.14835419 0.12264283 0.13219621 0.11796859 0.11562724 0.12225846\n",
      " 0.07415138], B intercept: 0.00027712403087754657, Train loss: 0.19903, Test loss: 0.20467\n",
      "\n",
      "-- Epoch no(iteration no)  19\n",
      "W intercept: [0.15519622 0.1281926  0.1394748  0.12425022 0.11963092 0.12620477\n",
      " 0.07607379], B intercept: 0.002180202533371299, Train loss: 0.19549, Test loss: 0.20125\n",
      "\n",
      "-- Epoch no(iteration no)  20\n",
      "W intercept: [0.16441057 0.1330913  0.14569741 0.1291094  0.12378506 0.13014236\n",
      " 0.07831607], B intercept: 0.006338097118072018, Train loss: 0.19203, Test loss: 0.19797\n",
      "\n",
      "-- Epoch no(iteration no)  21\n",
      "W intercept: [0.17215745 0.13730875 0.15402016 0.13421049 0.1300109  0.13426365\n",
      " 0.08056883], B intercept: 0.009029755589193618, Train loss: 0.18844, Test loss: 0.19454\n",
      "\n",
      "-- Epoch no(iteration no)  22\n",
      "W intercept: [0.17863215 0.14253078 0.15990477 0.13916005 0.13592897 0.13747849\n",
      " 0.08608895], B intercept: 0.00720655149817515, Train loss: 0.18513, Test loss: 0.19138\n",
      "\n",
      "-- Epoch no(iteration no)  23\n",
      "W intercept: [0.18531032 0.14915594 0.16698626 0.14394999 0.13942659 0.14407161\n",
      " 0.08766935], B intercept: 0.0033065573724124627, Train loss: 0.18174, Test loss: 0.18804\n",
      "\n",
      "-- Epoch no(iteration no)  24\n",
      "W intercept: [0.19135106 0.15587628 0.17053307 0.1480503  0.1440353  0.14798278\n",
      " 0.09004063], B intercept: 0.0023276296512523666, Train loss: 0.17898, Test loss: 0.18535\n",
      "\n",
      "-- Epoch no(iteration no)  25\n",
      "W intercept: [0.19839641 0.16237052 0.174542   0.15374936 0.15039617 0.15306621\n",
      " 0.09316385], B intercept: -0.0014360756372037789, Train loss: 0.17573, Test loss: 0.18218\n",
      "\n",
      "-- Epoch no(iteration no)  26\n",
      "W intercept: [0.20453585 0.17193708 0.17865291 0.15723157 0.15612369 0.15790061\n",
      " 0.09418395], B intercept: -0.0012410969212286065, Train loss: 0.17271, Test loss: 0.17925\n",
      "\n",
      "-- Epoch no(iteration no)  27\n",
      "W intercept: [0.20948894 0.1792699  0.18307715 0.163451   0.16085109 0.16233253\n",
      " 0.09753991], B intercept: -0.005419868884419825, Train loss: 0.16983, Test loss: 0.17639\n",
      "\n",
      "-- Epoch no(iteration no)  28\n",
      "W intercept: [0.21240598 0.18554199 0.18756132 0.16787782 0.16561075 0.16475297\n",
      " 0.09843542], B intercept: -0.005662658722680608, Train loss: 0.16769, Test loss: 0.17428\n",
      "\n",
      "-- Epoch no(iteration no)  29\n",
      "W intercept: [0.21650141 0.18853953 0.19381668 0.17260628 0.16590402 0.169955\n",
      " 0.10174046], B intercept: -0.0011247774895619283, Train loss: 0.16561, Test loss: 0.17234\n",
      "\n",
      "-- Epoch no(iteration no)  30\n",
      "W intercept: [0.22254838 0.18998347 0.19924026 0.17526017 0.17027395 0.17405132\n",
      " 0.10628671], B intercept: -0.00048630839410127393, Train loss: 0.16345, Test loss: 0.17032\n",
      "\n",
      "-- Epoch no(iteration no)  31\n",
      "W intercept: [0.22891142 0.19285319 0.2036206  0.17805762 0.17449974 0.17918707\n",
      " 0.10917137], B intercept: -0.0021015523458121875, Train loss: 0.16124, Test loss: 0.16821\n",
      "\n",
      "-- Epoch no(iteration no)  32\n",
      "W intercept: [0.23459755 0.19713369 0.20756387 0.18437996 0.17992032 0.18489198\n",
      " 0.11057666], B intercept: -0.002167858297816623, Train loss: 0.15876, Test loss: 0.16580\n",
      "\n",
      "-- Epoch no(iteration no)  33\n",
      "W intercept: [0.2412689  0.20028795 0.21304421 0.18759571 0.18183835 0.18810703\n",
      " 0.1121135 ], B intercept: -0.0007405061784965377, Train loss: 0.15681, Test loss: 0.16392\n",
      "\n",
      "-- Epoch no(iteration no)  34\n",
      "W intercept: [0.24805636 0.20437214 0.21807291 0.19244509 0.18537572 0.19002201\n",
      " 0.11339233], B intercept: 0.0026417023009869776, Train loss: 0.15472, Test loss: 0.16192\n",
      "\n",
      "-- Epoch no(iteration no)  35\n",
      "W intercept: [0.25401499 0.20740707 0.22375914 0.19529294 0.19082542 0.19243526\n",
      " 0.11499038], B intercept: 0.00450594157313776, Train loss: 0.15274, Test loss: 0.16002\n",
      "\n",
      "-- Epoch no(iteration no)  36\n",
      "W intercept: [0.25902273 0.21170331 0.22913672 0.19946321 0.1949693  0.19601021\n",
      " 0.11916742], B intercept: 0.0020413719421971035, Train loss: 0.15062, Test loss: 0.15795\n",
      "\n",
      "-- Epoch no(iteration no)  37\n",
      "W intercept: [0.26445297 0.21677127 0.23472196 0.20306713 0.19671402 0.20032106\n",
      " 0.1204521 ], B intercept: -3.5311964089355333e-05, Train loss: 0.14867, Test loss: 0.15600\n",
      "\n",
      "-- Epoch no(iteration no)  38\n",
      "W intercept: [0.26908221 0.22204411 0.23718792 0.20650731 0.20097787 0.20391405\n",
      " 0.12292377], B intercept: -0.0018584123472558823, Train loss: 0.14690, Test loss: 0.15427\n",
      "\n",
      "-- Epoch no(iteration no)  39\n",
      "W intercept: [0.27471096 0.2279473  0.24114176 0.21082863 0.20554255 0.20770664\n",
      " 0.12419713], B intercept: -0.003424525823712975, Train loss: 0.14487, Test loss: 0.15226\n",
      "\n",
      "-- Epoch no(iteration no)  40\n",
      "W intercept: [0.27995726 0.23535312 0.24320929 0.21379236 0.21012926 0.21191755\n",
      " 0.12562412], B intercept: -0.005267394639751273, Train loss: 0.14300, Test loss: 0.15042\n",
      "\n",
      "-- Epoch no(iteration no)  41\n",
      "W intercept: [0.28423575 0.24150352 0.24660529 0.21860635 0.21444779 0.21494479\n",
      " 0.128099  ], B intercept: -0.008070287491528652, Train loss: 0.14117, Test loss: 0.14859\n",
      "\n",
      "-- Epoch no(iteration no)  42\n",
      "W intercept: [0.28614432 0.24615048 0.25092803 0.22230479 0.21798286 0.21661489\n",
      " 0.12867372], B intercept: -0.007696411694868277, Train loss: 0.13985, Test loss: 0.14728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 69/300 [00:00<00:00, 232.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Epoch no(iteration no)  43\n",
      "W intercept: [0.2893827  0.24656061 0.25730741 0.22617163 0.21739403 0.2207674\n",
      " 0.13213185], B intercept: -0.003959490739183488, Train loss: 0.13858, Test loss: 0.14611\n",
      "\n",
      "-- Epoch no(iteration no)  44\n",
      "W intercept: [0.29484117 0.24879701 0.2605689  0.22817188 0.22061861 0.2249628\n",
      " 0.13526226], B intercept: -0.002256403519432138, Train loss: 0.13715, Test loss: 0.14478\n",
      "\n",
      "-- Epoch no(iteration no)  45\n",
      "W intercept: [0.30038267 0.25138897 0.26373043 0.23035004 0.22460194 0.2283681\n",
      " 0.13671989], B intercept: -0.0033919787626388886, Train loss: 0.13574, Test loss: 0.14342\n",
      "\n",
      "-- Epoch no(iteration no)  46\n",
      "W intercept: [0.30417468 0.25461287 0.26729546 0.23496852 0.228068   0.2309335\n",
      " 0.1372915 ], B intercept: -0.002212896195881225, Train loss: 0.13440, Test loss: 0.14210\n",
      "\n",
      "-- Epoch no(iteration no)  47\n",
      "W intercept: [0.31095986 0.25731314 0.27176328 0.2365812  0.22985067 0.23433795\n",
      " 0.13930736], B intercept: -0.0012703139433941437, Train loss: 0.13298, Test loss: 0.14075\n",
      "\n",
      "-- Epoch no(iteration no)  48\n",
      "W intercept: [0.3150861  0.2606364  0.27588899 0.24156138 0.23294186 0.23671709\n",
      " 0.14083533], B intercept: 0.0008390539420594414, Train loss: 0.13158, Test loss: 0.13940\n",
      "\n",
      "-- Epoch no(iteration no)  49\n",
      "W intercept: [0.32046835 0.26217988 0.28028663 0.24321827 0.23717032 0.23755924\n",
      " 0.14293909], B intercept: 0.0031867154511121213, Train loss: 0.13038, Test loss: 0.13828\n",
      "\n",
      "-- Epoch no(iteration no)  50\n",
      "W intercept: [0.32502314 0.26630467 0.28504062 0.2465326  0.24034251 0.24115086\n",
      " 0.14437459], B intercept: 0.0002904840503194042, Train loss: 0.12892, Test loss: 0.13680\n",
      "\n",
      "-- Epoch no(iteration no)  51\n",
      "W intercept: [0.32946772 0.27058386 0.28935177 0.25008994 0.24218222 0.24429594\n",
      " 0.14596268], B intercept: -0.0015972094408002713, Train loss: 0.12758, Test loss: 0.13546\n",
      "\n",
      "-- Epoch no(iteration no)  52\n",
      "W intercept: [0.33370838 0.27536251 0.29135042 0.2522961  0.24534584 0.24684188\n",
      " 0.14752676], B intercept: -0.0032981795852156555, Train loss: 0.12643, Test loss: 0.13431\n",
      "\n",
      "-- Epoch no(iteration no)  53\n",
      "W intercept: [0.33833109 0.28004542 0.29432507 0.25662346 0.2485337  0.25021744\n",
      " 0.14871907], B intercept: -0.0049089235368482715, Train loss: 0.12507, Test loss: 0.13295\n",
      "\n",
      "-- Epoch no(iteration no)  54\n",
      "W intercept: [0.34168567 0.28632982 0.29631671 0.25931878 0.25259742 0.25405229\n",
      " 0.15014029], B intercept: -0.005884703021915857, Train loss: 0.12380, Test loss: 0.13170\n",
      "\n",
      "-- Epoch no(iteration no)  55\n",
      "W intercept: [0.3456263  0.29121347 0.29887418 0.2629163  0.25633486 0.25621213\n",
      " 0.15239791], B intercept: -0.00842910924416771, Train loss: 0.12258, Test loss: 0.13047\n",
      "\n",
      "-- Epoch no(iteration no)  56\n",
      "W intercept: [0.34723039 0.29528617 0.30263247 0.2663758  0.25826923 0.25791299\n",
      " 0.15235132], B intercept: -0.007252183484429887, Train loss: 0.12168, Test loss: 0.12958\n",
      "\n",
      "-- Epoch no(iteration no)  57\n",
      "W intercept: [0.35015971 0.29495448 0.30842438 0.26937817 0.25837433 0.26080982\n",
      " 0.15563897], B intercept: -0.004547493307365065, Train loss: 0.12080, Test loss: 0.12876\n",
      "\n",
      "-- Epoch no(iteration no)  58\n",
      "W intercept: [0.35480876 0.29691592 0.31092287 0.2704166  0.26105983 0.26478907\n",
      " 0.15810861], B intercept: -0.002799537656963477, Train loss: 0.11981, Test loss: 0.12787\n",
      "\n",
      "-- Epoch no(iteration no)  59\n",
      "W intercept: [0.35950405 0.29898354 0.3135097  0.272754   0.26440693 0.26703939\n",
      " 0.15917636], B intercept: -0.0040347986047314386, Train loss: 0.11885, Test loss: 0.12692\n",
      "\n",
      "-- Epoch no(iteration no)  60\n",
      "W intercept: [0.36279028 0.30115938 0.31695427 0.27591042 0.26681134 0.26880799\n",
      " 0.15997523], B intercept: -0.002446514940582148, Train loss: 0.11797, Test loss: 0.12607\n",
      "\n",
      "-- Epoch no(iteration no)  61\n",
      "W intercept: [0.36916565 0.30366106 0.32029253 0.27682801 0.26805406 0.27116209\n",
      " 0.1605875 ], B intercept: -0.0010290440583250594, Train loss: 0.11702, Test loss: 0.12516\n",
      "\n",
      "-- Epoch no(iteration no)  62\n",
      "W intercept: [0.37240373 0.30623958 0.32469326 0.28154958 0.27101869 0.2732006\n",
      " 0.16184767], B intercept: 0.0005363608946789534, Train loss: 0.11596, Test loss: 0.12414\n",
      "\n",
      "-- Epoch no(iteration no)  63\n",
      "W intercept: [0.3767093  0.30792913 0.32826286 0.28280013 0.27461408 0.27403112\n",
      " 0.16416126], B intercept: 0.001519280715413275, Train loss: 0.11510, Test loss: 0.12333\n",
      "\n",
      "-- Epoch no(iteration no)  64\n",
      "W intercept: [0.38110578 0.31140152 0.33222749 0.28525701 0.27671693 0.27669618\n",
      " 0.16528322], B intercept: -0.0007792040290909969, Train loss: 0.11410, Test loss: 0.12230\n",
      "\n",
      "-- Epoch no(iteration no)  65\n",
      "W intercept: [0.38383282 0.31439476 0.33526576 0.28784592 0.27901832 0.27921416\n",
      " 0.16628911], B intercept: -0.0009303144453574522, Train loss: 0.11328, Test loss: 0.12149\n",
      "\n",
      "-- Epoch no(iteration no)  66\n",
      "W intercept: [0.38816297 0.31892564 0.33697311 0.29066932 0.28139149 0.2818679\n",
      " 0.16793631], B intercept: -0.003875077358974013, Train loss: 0.11233, Test loss: 0.12052\n",
      "\n",
      "-- Epoch no(iteration no)  67\n",
      "W intercept: [0.39208546 0.32423691 0.33900468 0.29317766 0.2840502  0.28468878\n",
      " 0.16862541], B intercept: -0.00406015334294159, Train loss: 0.11138, Test loss: 0.11959\n",
      "\n",
      "-- Epoch no(iteration no)  68\n",
      "W intercept: [0.39530566 0.32867385 0.34145113 0.29572904 0.28745359 0.28674325\n",
      " 0.17048133], B intercept: -0.006211016570766062, Train loss: 0.11048, Test loss: 0.11867\n",
      "\n",
      "-- Epoch no(iteration no)  69\n",
      "W intercept: [0.39644399 0.3333519  0.34367346 0.29970873 0.29024251 0.28864105\n",
      " 0.1713807 ], B intercept: -0.008504898442720068, Train loss: 0.10971, Test loss: 0.11787\n",
      "\n",
      "-- Epoch no(iteration no)  70\n",
      "W intercept: [0.39871172 0.3350041  0.34793014 0.30261864 0.29094782 0.28979071\n",
      " 0.17302234], B intercept: -0.00436787628659052, Train loss: 0.10904, Test loss: 0.11726\n",
      "\n",
      "-- Epoch no(iteration no)  71\n",
      "W intercept: [0.40138033 0.33525895 0.35180807 0.3047905  0.29158343 0.29287188\n",
      " 0.17600809], B intercept: -0.0024536968974342194, Train loss: 0.10838, Test loss: 0.11666\n",
      "\n",
      "-- Epoch no(iteration no)  72\n",
      "W intercept: [0.40670239 0.33667098 0.35468715 0.30470375 0.29433858 0.29528737\n",
      " 0.17701725], B intercept: -0.0021227926587571388, Train loss: 0.10764, Test loss: 0.11597\n",
      "\n",
      "-- Epoch no(iteration no)  73\n",
      "W intercept: [0.41022607 0.3385885  0.35672172 0.30789151 0.29707616 0.29900159\n",
      " 0.17794161], B intercept: -0.003394187989299512, Train loss: 0.10686, Test loss: 0.11520\n",
      "\n",
      "-- Epoch no(iteration no)  74\n",
      "W intercept: [0.41366553 0.34040274 0.35969315 0.31023529 0.29854075 0.29980273\n",
      " 0.17849264], B intercept: -0.002230917794422631, Train loss: 0.10625, Test loss: 0.11460\n",
      "\n",
      "-- Epoch no(iteration no)  75\n",
      "W intercept: [0.4187901  0.34260105 0.36225724 0.31166878 0.29984912 0.30155773\n",
      " 0.17889185], B intercept: 0.00017005978370971978, Train loss: 0.10555, Test loss: 0.11395\n",
      "\n",
      "-- Epoch no(iteration no)  76\n",
      "W intercept: [0.42258639 0.3446231  0.36652682 0.31468042 0.30272736 0.3033632\n",
      " 0.17954886], B intercept: 0.002020811404450923, Train loss: 0.10473, Test loss: 0.11318\n",
      "\n",
      "-- Epoch no(iteration no)  77\n",
      "W intercept: [0.42552622 0.34647168 0.36935256 0.31625696 0.30556849 0.30392122\n",
      " 0.18204319], B intercept: 0.001729223437328772, Train loss: 0.10412, Test loss: 0.11258\n",
      "\n",
      "-- Epoch no(iteration no)  78\n",
      "W intercept: [0.42879402 0.34945349 0.37309371 0.31875898 0.3069436  0.30679957\n",
      " 0.18282115], B intercept: -0.00019094881168706985, Train loss: 0.10337, Test loss: 0.11181\n",
      "\n",
      "-- Epoch no(iteration no)  79\n",
      "W intercept: [0.43181297 0.35258838 0.37493506 0.3204004  0.30820015 0.30827152\n",
      " 0.18380422], B intercept: -0.00031494150010592987, Train loss: 0.10281, Test loss: 0.11125\n",
      "\n",
      "-- Epoch no(iteration no)  80\n",
      "W intercept: [0.4353545  0.3561243  0.37665424 0.32309908 0.31148081 0.31074301\n",
      " 0.18520666], B intercept: -0.0028013201969123757, Train loss: 0.10206, Test loss: 0.11049\n",
      "\n",
      "-- Epoch no(iteration no)  81\n",
      "W intercept: [0.43853292 0.36113639 0.37852225 0.32475161 0.31395178 0.3132475\n",
      " 0.18563768], B intercept: -0.0022681377840798036, Train loss: 0.10136, Test loss: 0.10980\n",
      "\n",
      "-- Epoch no(iteration no)  82\n",
      "W intercept: [0.44137703 0.36489632 0.38065447 0.32770538 0.31626006 0.31515981\n",
      " 0.18756338], B intercept: -0.004912663489461667, Train loss: 0.10067, Test loss: 0.10909\n",
      "\n",
      "-- Epoch no(iteration no)  83\n",
      "W intercept: [0.44161986 0.36884552 0.38319591 0.33022267 0.31847131 0.31594114\n",
      " 0.18758132], B intercept: -0.004977484382337468, Train loss: 0.10019, Test loss: 0.10860\n",
      "\n",
      "-- Epoch no(iteration no)  84\n",
      "W intercept: [0.4437842  0.37007844 0.38740286 0.33310856 0.31825012 0.31757659\n",
      " 0.1895803 ], B intercept: -0.0008200637765647797, Train loss: 0.09964, Test loss: 0.10811\n",
      "\n",
      "-- Epoch no(iteration no)  85\n",
      "W intercept: [0.44739069 0.37001804 0.3904375  0.33472554 0.3202421  0.31980481\n",
      " 0.19214809], B intercept: 0.0003166273582853931, Train loss: 0.09907, Test loss: 0.10760\n",
      "\n",
      "-- Epoch no(iteration no)  86\n",
      "W intercept: [0.45141092 0.371356   0.39267644 0.33519777 0.32241593 0.3220452\n",
      " 0.19334092], B intercept: -0.0008499805152481028, Train loss: 0.09853, Test loss: 0.10707\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch no(iteration no)  87\n",
      "W intercept: [0.45440831 0.37308021 0.3943213  0.33797959 0.32479767 0.32510366\n",
      " 0.19383061], B intercept: -0.0010609946743379318, Train loss: 0.09796, Test loss: 0.10652\n",
      "\n",
      "-- Epoch no(iteration no)  88\n",
      "W intercept: [0.45857717 0.37419536 0.39705263 0.33925901 0.32550803 0.32641936\n",
      " 0.19438345], B intercept: 0.0001063006369913796, Train loss: 0.09746, Test loss: 0.10605\n",
      "\n",
      "-- Epoch no(iteration no)  89\n",
      "W intercept: [0.46239724 0.37664073 0.39943983 0.34167985 0.32657988 0.32694144\n",
      " 0.19440128], B intercept: 0.002627053277740103, Train loss: 0.09693, Test loss: 0.10555\n",
      "\n",
      "-- Epoch no(iteration no)  90\n",
      "W intercept: [0.46598254 0.37808373 0.40309037 0.34291624 0.33018567 0.32781377\n",
      " 0.19485048], B intercept: 0.0038420663432701474, Train loss: 0.09634, Test loss: 0.10499\n",
      "\n",
      "-- Epoch no(iteration no)  91\n",
      "W intercept: [0.46832926 0.37990648 0.40565006 0.34487734 0.33197479 0.32944562\n",
      " 0.19737428], B intercept: 0.003014021640912487, Train loss: 0.09584, Test loss: 0.10449\n",
      "\n",
      "-- Epoch no(iteration no)  92\n",
      "W intercept: [0.47145396 0.38203912 0.4093515  0.34681667 0.33251775 0.33150001\n",
      " 0.1982319 ], B intercept: 0.002025526984129085, Train loss: 0.09529, Test loss: 0.10394\n",
      "\n",
      "-- Epoch no(iteration no)  93\n",
      "W intercept: [0.47399353 0.38522466 0.4102371  0.34863294 0.3342426  0.33316158\n",
      " 0.19919959], B intercept: 0.0011227075975126413, Train loss: 0.09483, Test loss: 0.10348\n",
      "\n",
      "-- Epoch no(iteration no)  94\n",
      "W intercept: [0.47738108 0.38819716 0.41212842 0.35075741 0.33665765 0.33537323\n",
      " 0.20008343], B intercept: -0.00014963042310312354, Train loss: 0.09426, Test loss: 0.10290\n",
      "\n",
      "-- Epoch no(iteration no)  95\n",
      "W intercept: [0.48004499 0.39305972 0.41335332 0.35246464 0.33912694 0.33764947\n",
      " 0.20063761], B intercept: -0.0007542610207321029, Train loss: 0.09370, Test loss: 0.10234\n",
      "\n",
      "-- Epoch no(iteration no)  96\n",
      "W intercept: [0.48265203 0.39660633 0.41497184 0.35514303 0.34162634 0.33906978\n",
      " 0.20210641], B intercept: -0.00249172756334732, Train loss: 0.09317, Test loss: 0.10179\n",
      "\n",
      "-- Epoch no(iteration no)  97\n",
      "W intercept: [0.48269093 0.39959665 0.41779124 0.35747352 0.34306695 0.33939407\n",
      " 0.20212851], B intercept: -0.0022571817138371474, Train loss: 0.09282, Test loss: 0.10143\n",
      "\n",
      "-- Epoch no(iteration no)  98\n",
      "W intercept: [0.48469317 0.3998781  0.42254153 0.35994213 0.34230822 0.34122252\n",
      " 0.20461728], B intercept: 0.0016507365842130999, Train loss: 0.09236, Test loss: 0.10104\n",
      "\n",
      "-- Epoch no(iteration no) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 128/300 [00:00<00:00, 268.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99\n",
      "W intercept: [0.48798597 0.40078611 0.42413338 0.36107256 0.34428705 0.34374918\n",
      " 0.20641009], B intercept: 0.002759617271316043, Train loss: 0.09191, Test loss: 0.10064\n",
      "\n",
      "-- Epoch no(iteration no)  100\n",
      "W intercept: [0.49194227 0.40151681 0.42625728 0.36124577 0.3464314  0.34530892\n",
      " 0.2071669 ], B intercept: 0.002098030840441855, Train loss: 0.09150, Test loss: 0.10024\n",
      "\n",
      "-- Epoch no(iteration no)  101\n",
      "W intercept: [0.49397863 0.40272188 0.42752837 0.36344721 0.34814518 0.3463449\n",
      " 0.20726054], B intercept: 0.00279344907996895, Train loss: 0.09117, Test loss: 0.09992\n",
      "\n",
      "-- Epoch no(iteration no)  102\n",
      "W intercept: [0.49823059 0.40406405 0.43045695 0.36472509 0.34908621 0.34836563\n",
      " 0.20837255], B intercept: 0.003119194995081262, Train loss: 0.09067, Test loss: 0.09945\n",
      "\n",
      "-- Epoch no(iteration no)  103\n",
      "W intercept: [0.50109326 0.40638108 0.43246007 0.3672577  0.35022814 0.3493372\n",
      " 0.20828616], B intercept: 0.005230287945329863, Train loss: 0.09024, Test loss: 0.09905\n",
      "\n",
      "-- Epoch no(iteration no)  104\n",
      "W intercept: [0.50453054 0.40705315 0.435444   0.36791065 0.35298799 0.34942133\n",
      " 0.20992001], B intercept: 0.00725921500108864, Train loss: 0.08982, Test loss: 0.09868\n",
      "\n",
      "-- Epoch no(iteration no)  105\n",
      "W intercept: [0.50711728 0.40926639 0.43789256 0.36981139 0.35476231 0.35127897\n",
      " 0.21078921], B intercept: 0.005474510120983976, Train loss: 0.08937, Test loss: 0.09822\n",
      "\n",
      "-- Epoch no(iteration no)  106\n",
      "W intercept: [0.50975765 0.41133127 0.44104005 0.37180655 0.35519549 0.35306176\n",
      " 0.21184185], B intercept: 0.004443815251083582, Train loss: 0.08894, Test loss: 0.09777\n",
      "\n",
      "-- Epoch no(iteration no)  107\n",
      "W intercept: [0.51231383 0.41451773 0.44197433 0.37295548 0.35677584 0.35424653\n",
      " 0.21234178], B intercept: 0.004003531894835586, Train loss: 0.08857, Test loss: 0.09740\n",
      "\n",
      "-- Epoch no(iteration no)  108\n",
      "W intercept: [0.51507951 0.4171492  0.4437818  0.3755922  0.35847494 0.35630213\n",
      " 0.2131375 ], B intercept: 0.002862135775757357, Train loss: 0.08811, Test loss: 0.09694\n",
      "\n",
      "-- Epoch no(iteration no)  109\n",
      "W intercept: [0.51710467 0.42131671 0.4446751  0.37710161 0.36098317 0.35866639\n",
      " 0.21386251], B intercept: 0.0023648570271888295, Train loss: 0.08768, Test loss: 0.09651\n",
      "\n",
      "-- Epoch no(iteration no)  110\n",
      "W intercept: [0.51974115 0.42428438 0.44607648 0.37899011 0.36341692 0.35952271\n",
      " 0.21525493], B intercept: 0.0004275540332639422, Train loss: 0.08727, Test loss: 0.09608\n",
      "\n",
      "-- Epoch no(iteration no)  111\n",
      "W intercept: [0.51982128 0.42745824 0.44843412 0.3814572  0.36445321 0.36005511\n",
      " 0.21491234], B intercept: 0.0014417216114551222, Train loss: 0.08697, Test loss: 0.09577\n",
      "\n",
      "-- Epoch no(iteration no)  112\n",
      "W intercept: [0.52174429 0.42675612 0.45316311 0.38368849 0.36388836 0.36156457\n",
      " 0.21750702], B intercept: 0.0046309279300175725, Train loss: 0.08660, Test loss: 0.09546\n",
      "\n",
      "-- Epoch no(iteration no)  113\n",
      "W intercept: [0.5245841  0.42780531 0.45484802 0.38460789 0.36574735 0.36378617\n",
      " 0.21933352], B intercept: 0.005675365983364617, Train loss: 0.08623, Test loss: 0.09514\n",
      "\n",
      "-- Epoch no(iteration no)  114\n",
      "W intercept: [0.52814612 0.42869673 0.45610241 0.38484452 0.3676196  0.3649637\n",
      " 0.21964011], B intercept: 0.0053714218941259256, Train loss: 0.08592, Test loss: 0.09484\n",
      "\n",
      "-- Epoch no(iteration no)  115\n",
      "W intercept: [0.53019289 0.42958761 0.45755912 0.38653863 0.36897437 0.36584552\n",
      " 0.21999375], B intercept: 0.005981702230851015, Train loss: 0.08564, Test loss: 0.09458\n",
      "\n",
      "-- Epoch no(iteration no)  116\n",
      "W intercept: [0.53483223 0.4312572  0.46023017 0.38672343 0.36876402 0.36709481\n",
      " 0.22019067], B intercept: 0.007425015951240542, Train loss: 0.08526, Test loss: 0.09423\n",
      "\n",
      "-- Epoch no(iteration no)  117\n",
      "W intercept: [0.53708632 0.43295546 0.46253102 0.3894934  0.3709957  0.36803669\n",
      " 0.22069447], B intercept: 0.00855069208483499, Train loss: 0.08487, Test loss: 0.09387\n",
      "\n",
      "-- Epoch no(iteration no)  118\n",
      "W intercept: [0.53965531 0.43353229 0.46496284 0.39034923 0.37329749 0.36832885\n",
      " 0.2222059 ], B intercept: 0.009926400444638437, Train loss: 0.08455, Test loss: 0.09358\n",
      "\n",
      "-- Epoch no(iteration no)  119\n",
      "W intercept: [0.54218956 0.43560479 0.46698706 0.39209171 0.37476884 0.36994148\n",
      " 0.22306102], B intercept: 0.007997770617927757, Train loss: 0.08419, Test loss: 0.09320\n",
      "\n",
      "-- Epoch no(iteration no)  120\n",
      "W intercept: [0.54409486 0.43706046 0.4696099  0.39327465 0.37578883 0.37124297\n",
      " 0.22338367], B intercept: 0.008881372617867497, Train loss: 0.08388, Test loss: 0.09291\n",
      "\n",
      "-- Epoch no(iteration no)  121\n",
      "W intercept: [0.54695825 0.44024477 0.47065829 0.39512644 0.37675854 0.37282936\n",
      " 0.22442854], B intercept: 0.006792295506852442, Train loss: 0.08352, Test loss: 0.09253\n",
      "\n",
      "-- Epoch no(iteration no)  122\n",
      "W intercept: [0.54938651 0.44391303 0.47192379 0.39674108 0.37828692 0.37472568\n",
      " 0.22482469], B intercept: 0.006564460672279969, Train loss: 0.08315, Test loss: 0.09216\n",
      "\n",
      "-- Epoch no(iteration no)  123\n",
      "W intercept: [0.55153831 0.44677491 0.47324139 0.39823085 0.38056093 0.37574536\n",
      " 0.22593149], B intercept: 0.004973110539535263, Train loss: 0.08281, Test loss: 0.09181\n",
      "\n",
      "-- Epoch no(iteration no)  124\n",
      "W intercept: [0.5516722  0.44996463 0.4745359  0.40085808 0.38219203 0.37645623\n",
      " 0.22720965], B intercept: 0.0034451692378669753, Train loss: 0.08254, Test loss: 0.09151\n",
      "\n",
      "-- Epoch no(iteration no)  125\n",
      "W intercept: [0.55303442 0.45118588 0.47759159 0.40259382 0.38272332 0.3768429\n",
      " 0.22794045], B intercept: 0.006829260598992583, Train loss: 0.08226, Test loss: 0.09128\n",
      "\n",
      "-- Epoch no(iteration no)  126\n",
      "W intercept: [0.55505164 0.45118389 0.48076223 0.4047056  0.3826252  0.37889276\n",
      " 0.22868635], B intercept: 0.009091520432613137, Train loss: 0.08196, Test loss: 0.09102\n",
      "\n",
      "-- Epoch no(iteration no)  127\n",
      "W intercept: [0.55897661 0.45170468 0.48267797 0.40384993 0.38445282 0.38007531\n",
      " 0.23057698], B intercept: 0.009654122696628138, Train loss: 0.08165, Test loss: 0.09075\n",
      "\n",
      "-- Epoch no(iteration no)  128\n",
      "W intercept: [0.56129314 0.45287349 0.48405118 0.40577283 0.38608193 0.38248163\n",
      " 0.23108636], B intercept: 0.008797331862508839, Train loss: 0.08135, Test loss: 0.09045\n",
      "\n",
      "-- Epoch no(iteration no)  129\n",
      "W intercept: [0.56345124 0.45389971 0.48533191 0.40710464 0.38728736 0.38303082\n",
      " 0.23157322], B intercept: 0.009083906861167945, Train loss: 0.08112, Test loss: 0.09023\n",
      "\n",
      "-- Epoch no(iteration no)  130\n",
      "W intercept: [0.56758087 0.45524942 0.48721482 0.40778303 0.38729061 0.38370951\n",
      " 0.23127136], B intercept: 0.01170300901288797, Train loss: 0.08082, Test loss: 0.08998\n",
      "\n",
      "-- Epoch no(iteration no)  131\n",
      "W intercept: [0.57022616 0.45647476 0.49013617 0.40984128 0.38926629 0.38480397\n",
      " 0.23153114], B intercept: 0.013121178451220641, Train loss: 0.08046, Test loss: 0.08965\n",
      "\n",
      "-- Epoch no(iteration no)  132\n",
      "W intercept: [0.57193291 0.45740699 0.49198246 0.41053785 0.3912549  0.38483148\n",
      " 0.23335852], B intercept: 0.01318569381920847, Train loss: 0.08023, Test loss: 0.08943\n",
      "\n",
      "-- Epoch no(iteration no)  133\n",
      "W intercept: [0.57415058 0.45928717 0.49457605 0.41235745 0.39204901 0.38670499\n",
      " 0.23388071], B intercept: 0.01189648253076576, Train loss: 0.07991, Test loss: 0.08910\n",
      "\n",
      "-- Epoch no(iteration no)  134\n",
      "W intercept: [0.57608833 0.46121313 0.49600939 0.4131746  0.39235168 0.38734218\n",
      " 0.23444691], B intercept: 0.012223880602432057, Train loss: 0.07969, Test loss: 0.08888\n",
      "\n",
      "-- Epoch no(iteration no)  135\n",
      "W intercept: [0.57851671 0.46366881 0.49704857 0.41492739 0.39445133 0.38900988\n",
      " 0.23535414], B intercept: 0.01048423042028015, Train loss: 0.07938, Test loss: 0.08856\n",
      "\n",
      "-- Epoch no(iteration no)  136\n",
      "W intercept: [0.58079652 0.4671199  0.49824786 0.41595951 0.39579334 0.39069529\n",
      " 0.23569379], B intercept: 0.010640748547963022, Train loss: 0.07907, Test loss: 0.08826\n",
      "\n",
      "-- Epoch no(iteration no)  137\n",
      "W intercept: [0.58290591 0.46972765 0.49956025 0.41756372 0.39791289 0.3915839\n",
      " 0.23672002], B intercept: 0.008679721299060032, Train loss: 0.07878, Test loss: 0.08795\n",
      "\n",
      "-- Epoch no(iteration no)  138\n",
      "W intercept: [0.58215351 0.47248259 0.50106978 0.41988596 0.39933857 0.39213196\n",
      " 0.23701243], B intercept: 0.008558400280336912, Train loss: 0.07859, Test loss: 0.08775\n",
      "\n",
      "-- Epoch no(iteration no)  139\n",
      "W intercept: [0.58350005 0.47361823 0.50487191 0.42190746 0.39846973 0.39249997\n",
      " 0.23841816], B intercept: 0.012968931720372593, Train loss: 0.07832, Test loss: 0.08754\n",
      "\n",
      "-- Epoch no(iteration no)  140\n",
      "W intercept: [0.58619602 0.47324368 0.5070628  0.42308991 0.39978272 0.39418686\n",
      " 0.24036238], B intercept: 0.014087206086677066, Train loss: 0.07805, Test loss: 0.08731\n",
      "\n",
      "-- Epoch no(iteration no)  141\n",
      "W intercept: [0.58948948 0.47406365 0.50853537 0.42293854 0.40144721 0.39543355\n",
      " 0.24118528], B intercept: 0.013171054705190402, Train loss: 0.07780, Test loss: 0.08708\n",
      "\n",
      "-- Epoch no(iteration no)  142\n",
      "W intercept: [0.59160309 0.47500467 0.50953856 0.42454983 0.4028189  0.39750634\n",
      " 0.24144902], B intercept: 0.012932796489604095, Train loss: 0.07757, Test loss: 0.08685\n",
      "\n",
      "-- Epoch no(iteration no)  143\n",
      "W intercept: [0.59441609 0.47568716 0.51127953 0.42508315 0.40339895 0.39812771\n",
      " 0.24185409], B intercept: 0.013647038236556754, Train loss: 0.07735, Test loss: 0.08665\n",
      "\n",
      "-- Epoch no(iteration no)  144\n",
      "W intercept: [0.59756585 0.47741095 0.51276362 0.42679908 0.40356442 0.3983278\n",
      " 0.2413447 ], B intercept: 0.016095160361027656, Train loss: 0.07710, Test loss: 0.08644\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch no(iteration no)  145\n",
      "W intercept: [0.60000337 0.47825089 0.51506917 0.428023   0.40649729 0.39955949\n",
      " 0.24140471], B intercept: 0.01771744956883218, Train loss: 0.07681, Test loss: 0.08619\n",
      "\n",
      "-- Epoch no(iteration no)  146\n",
      "W intercept: [0.60166074 0.47920438 0.51699327 0.42877384 0.4074714  0.39959173\n",
      " 0.24359105], B intercept: 0.016724586886763298, Train loss: 0.07661, Test loss: 0.08599\n",
      "\n",
      "-- Epoch no(iteration no)  147\n",
      "W intercept: [0.6037823  0.48032964 0.5200618  0.43003038 0.40762637 0.40091727\n",
      " 0.24418284], B intercept: 0.016391527335594555, Train loss: 0.07636, Test loss: 0.08573\n",
      "\n",
      "-- Epoch no(iteration no)  148\n",
      "W intercept: [0.60553229 0.48275633 0.52051818 0.43128032 0.40845103 0.40190441\n",
      " 0.24471174], B intercept: 0.01586069918238476, Train loss: 0.07616, Test loss: 0.08553\n",
      "\n",
      "-- Epoch no(iteration no)  149\n",
      "W intercept: [0.60791724 0.4847719  0.52172009 0.43270862 0.41005146 0.40348816\n",
      " 0.24534881], B intercept: 0.014927718215608165, Train loss: 0.07590, Test loss: 0.08526\n",
      "\n",
      "-- Epoch no(iteration no)  150\n",
      "W intercept: [0.60976212 0.48853045 0.52245531 0.43385998 0.41179365 0.40513263\n",
      " 0.2456812 ], B intercept: 0.014558819209114414, Train loss: 0.07563, Test loss: 0.08500\n",
      "\n",
      "-- Epoch no(iteration no)  151\n",
      "W intercept: [0.61168253 0.49107749 0.52351214 0.43567471 0.41361218 0.40586488\n",
      " 0.24675914], B intercept: 0.013244644506685306, Train loss: 0.07538, Test loss: 0.08474\n",
      "\n",
      "-- Epoch no(iteration no)  152\n",
      "W intercept: [0.61107904 0.49339517 0.52568031 0.43751918 0.41441613 0.40553934\n",
      " 0.24658337], B intercept: 0.013635628060875033, Train loss: 0.07524, Test loss: 0.08458\n",
      "\n",
      "-- Epoch no(iteration no)  153\n",
      "W intercept: [0.61237021 0.49370232 0.52970415 0.43928629 0.4134839  0.40647772\n",
      " 0.24876049], B intercept: 0.017349676448572153, Train loss: 0.07499, Test loss: 0.08439\n",
      "\n",
      "-- Epoch no(iteration no)  154\n",
      "W intercept: [0.61491676 0.49391499 0.53071902 0.44015832 0.41496233 0.40842269\n",
      " 0.25008276], B intercept: 0.018441108086289692, Train loss: 0.07477, Test loss: 0.08422\n",
      "\n",
      "-- Epoch no(iteration no)  155\n",
      "W intercept: [0.6182082  0.49425883 0.53218803 0.43976022 0.41650919 0.40928552\n",
      " 0.25057936], B intercept: 0.017936317895886216, Train loss: 0.07457, Test loss: 0.08403\n",
      "\n",
      "-- Epoch no(iteration no) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 184/300 [00:00<00:00, 267.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 156\n",
      "W intercept: [0.6196975  0.49481986 0.53296417 0.4410695  0.41764803 0.40977783\n",
      " 0.25056218], B intercept: 0.018474391708548466, Train loss: 0.07442, Test loss: 0.08390\n",
      "\n",
      "-- Epoch no(iteration no)  157\n",
      "W intercept: [0.62283485 0.49573038 0.53476191 0.44203143 0.41821544 0.41129821\n",
      " 0.25125657], B intercept: 0.01845078466996508, Train loss: 0.07418, Test loss: 0.08367\n",
      "\n",
      "-- Epoch no(iteration no)  158\n",
      "W intercept: [0.62521101 0.49744928 0.53636737 0.44365154 0.41861416 0.41176742\n",
      " 0.25100775], B intercept: 0.020514475739320717, Train loss: 0.07396, Test loss: 0.08348\n",
      "\n",
      "-- Epoch no(iteration no)  159\n",
      "W intercept: [0.62770784 0.49780299 0.53862382 0.44401609 0.42092551 0.41160475\n",
      " 0.25228441], B intercept: 0.022319132241102393, Train loss: 0.07374, Test loss: 0.08331\n",
      "\n",
      "-- Epoch no(iteration no)  160\n",
      "W intercept: [0.6293621  0.49922526 0.54017786 0.44531414 0.42202066 0.41274315\n",
      " 0.25284459], B intercept: 0.021164840575121767, Train loss: 0.07354, Test loss: 0.08310\n",
      "\n",
      "-- Epoch no(iteration no)  161\n",
      "W intercept: [0.63137908 0.50040781 0.54288191 0.44664712 0.4221328  0.41395116\n",
      " 0.2536679 ], B intercept: 0.02052376644152629, Train loss: 0.07332, Test loss: 0.08287\n",
      "\n",
      "-- Epoch no(iteration no)  162\n",
      "W intercept: [0.63318636 0.50286114 0.54337743 0.44742984 0.42301246 0.41470742\n",
      " 0.25388941], B intercept: 0.020391186370984853, Train loss: 0.07315, Test loss: 0.08270\n",
      "\n",
      "-- Epoch no(iteration no)  163\n",
      "W intercept: [0.63520582 0.50475319 0.5446419  0.44935513 0.42414226 0.41622198\n",
      " 0.2545141 ], B intercept: 0.0194446322738559, Train loss: 0.07292, Test loss: 0.08247\n",
      "\n",
      "-- Epoch no(iteration no)  164\n",
      "W intercept: [0.63669968 0.50806288 0.54519237 0.45038737 0.42600485 0.4179729\n",
      " 0.25495785], B intercept: 0.019107159056395116, Train loss: 0.07270, Test loss: 0.08225\n",
      "\n",
      "-- Epoch no(iteration no)  165\n",
      "W intercept: [0.63861834 0.51023893 0.54607514 0.45166515 0.4278042  0.4183394\n",
      " 0.25600857], B intercept: 0.01766980003403538, Train loss: 0.07250, Test loss: 0.08204\n",
      "\n",
      "-- Epoch no(iteration no)  166\n",
      "W intercept: [0.63807685 0.51280212 0.54794559 0.45362415 0.42842998 0.41819304\n",
      " 0.25555768], B intercept: 0.01872925714501254, Train loss: 0.07236, Test loss: 0.08190\n",
      "\n",
      "-- Epoch no(iteration no)  167\n",
      "W intercept: [0.63954592 0.5121015  0.55205409 0.45538841 0.42772274 0.41917503\n",
      " 0.25787757], B intercept: 0.021760342125362733, Train loss: 0.07214, Test loss: 0.08174\n",
      "\n",
      "-- Epoch no(iteration no)  168\n",
      "W intercept: [0.64177927 0.51266048 0.55309546 0.45605349 0.42907198 0.4207725\n",
      " 0.25927656], B intercept: 0.022809137364838642, Train loss: 0.07195, Test loss: 0.08158\n",
      "\n",
      "-- Epoch no(iteration no)  169\n",
      "W intercept: [0.64466271 0.51320443 0.55405419 0.45569578 0.43039682 0.42138571\n",
      " 0.25944718], B intercept: 0.022610065383696466, Train loss: 0.07179, Test loss: 0.08144\n",
      "\n",
      "-- Epoch no(iteration no)  170\n",
      "W intercept: [0.64641603 0.51364402 0.55481356 0.45665379 0.43137285 0.42180155\n",
      " 0.25952983], B intercept: 0.022888143571109555, Train loss: 0.07167, Test loss: 0.08132\n",
      "\n",
      "-- Epoch no(iteration no)  171\n",
      "W intercept: [0.65007513 0.51491132 0.55682018 0.45671983 0.43088572 0.4226348\n",
      " 0.25959065], B intercept: 0.024481519487878575, Train loss: 0.07145, Test loss: 0.08114\n",
      "\n",
      "-- Epoch no(iteration no)  172\n",
      "W intercept: [0.65174491 0.51612711 0.55850415 0.45881931 0.43249525 0.42328432\n",
      " 0.2598372 ], B intercept: 0.025349045163109013, Train loss: 0.07125, Test loss: 0.08096\n",
      "\n",
      "-- Epoch no(iteration no)  173\n",
      "W intercept: [0.65356049 0.51636547 0.56033775 0.45929783 0.43436252 0.42326005\n",
      " 0.26112336], B intercept: 0.02662863679546745, Train loss: 0.07108, Test loss: 0.08083\n",
      "\n",
      "-- Epoch no(iteration no)  174\n",
      "W intercept: [0.65539831 0.51777133 0.5617248  0.46054633 0.43544945 0.42436975\n",
      " 0.26175836], B intercept: 0.025154610691839364, Train loss: 0.07090, Test loss: 0.08063\n",
      "\n",
      "-- Epoch no(iteration no)  175\n",
      "W intercept: [0.65690814 0.51866208 0.56379879 0.46181784 0.43662569 0.42543806\n",
      " 0.26218951], B intercept: 0.025535441097315162, Train loss: 0.07072, Test loss: 0.08046\n",
      "\n",
      "-- Epoch no(iteration no)  176\n",
      "W intercept: [0.65896655 0.52119263 0.56475714 0.46272357 0.43640363 0.42640928\n",
      " 0.26273318], B intercept: 0.024473324085009143, Train loss: 0.07055, Test loss: 0.08028\n",
      "\n",
      "-- Epoch no(iteration no)  177\n",
      "W intercept: [0.66071205 0.52412368 0.56558588 0.46381869 0.4375084  0.42779444\n",
      " 0.26295396], B intercept: 0.024483919433140128, Train loss: 0.07035, Test loss: 0.08009\n",
      "\n",
      "-- Epoch no(iteration no)  178\n",
      "W intercept: [0.66202018 0.52613537 0.56653948 0.46530153 0.43893732 0.42930675\n",
      " 0.26362818], B intercept: 0.02357464635610643, Train loss: 0.07017, Test loss: 0.07991\n",
      "\n",
      "-- Epoch no(iteration no)  179\n",
      "W intercept: [0.66204022 0.52880682 0.56739343 0.46695747 0.44034236 0.42852928\n",
      " 0.26485   ], B intercept: 0.02177465957246402, Train loss: 0.07005, Test loss: 0.07975\n",
      "\n",
      "-- Epoch no(iteration no)  180\n",
      "W intercept: [0.66296093 0.52961588 0.56993601 0.46842324 0.44062924 0.42845271\n",
      " 0.26548646], B intercept: 0.024916295216700694, Train loss: 0.06989, Test loss: 0.07964\n",
      "\n",
      "-- Epoch no(iteration no)  181\n",
      "W intercept: [0.66468486 0.52916569 0.57236799 0.47034211 0.44069593 0.42971025\n",
      " 0.26612801], B intercept: 0.027002006135765123, Train loss: 0.06971, Test loss: 0.07950\n",
      "\n",
      "-- Epoch no(iteration no)  182\n",
      "W intercept: [0.66741436 0.52993924 0.57414533 0.46928182 0.44154512 0.4308456\n",
      " 0.26771053], B intercept: 0.027405926782121226, Train loss: 0.06954, Test loss: 0.07936\n",
      "\n",
      "-- Epoch no(iteration no)  183\n",
      "W intercept: [0.66965382 0.53062608 0.57506422 0.47038168 0.4429818  0.43256857\n",
      " 0.26790554], B intercept: 0.027139727591975808, Train loss: 0.06937, Test loss: 0.07920\n",
      "\n",
      "-- Epoch no(iteration no)  184\n",
      "W intercept: [0.67132433 0.53115226 0.57586721 0.47109073 0.44381526 0.43271069\n",
      " 0.26818618], B intercept: 0.027477114592907453, Train loss: 0.06926, Test loss: 0.07910\n",
      "\n",
      "-- Epoch no(iteration no)  185\n",
      "W intercept: [0.67481093 0.53222821 0.57724759 0.47160176 0.44352201 0.43318937\n",
      " 0.26777534], B intercept: 0.02972698805997446, Train loss: 0.06908, Test loss: 0.07896\n",
      "\n",
      "-- Epoch no(iteration no)  186\n",
      "W intercept: [0.67682718 0.53312777 0.57943911 0.47316344 0.44502633 0.43396371\n",
      " 0.26786876], B intercept: 0.030914636275857076, Train loss: 0.06888, Test loss: 0.07879\n",
      "\n",
      "-- Epoch no(iteration no)  187\n",
      "W intercept: [0.67797107 0.53364493 0.58082086 0.47347585 0.44670508 0.43381723\n",
      " 0.26946176], B intercept: 0.03101771825557675, Train loss: 0.06876, Test loss: 0.07868\n",
      "\n",
      "-- Epoch no(iteration no)  188\n",
      "W intercept: [0.67960881 0.53493727 0.58292485 0.47492864 0.44716404 0.4351665\n",
      " 0.26979653], B intercept: 0.030190943302933995, Train loss: 0.06858, Test loss: 0.07850\n",
      "\n",
      "-- Epoch no(iteration no)  189\n",
      "W intercept: [0.68106847 0.53633186 0.58412141 0.47541956 0.44704536 0.43544131\n",
      " 0.27020246], B intercept: 0.03067692268151851, Train loss: 0.06847, Test loss: 0.07839\n",
      "\n",
      "-- Epoch no(iteration no)  190\n",
      "W intercept: [0.68298617 0.53827263 0.5848941  0.47675758 0.44864365 0.4367585\n",
      " 0.27091606], B intercept: 0.029231022624143527, Train loss: 0.06829, Test loss: 0.07821\n",
      "\n",
      "-- Epoch no(iteration no)  191\n",
      "W intercept: [0.68464004 0.5410862  0.58566319 0.47752455 0.44957949 0.43806758\n",
      " 0.27111462], B intercept: 0.029376603668164275, Train loss: 0.06812, Test loss: 0.07805\n",
      "\n",
      "-- Epoch no(iteration no)  192\n",
      "W intercept: [0.68630789 0.54311873 0.58659017 0.47857378 0.45127681 0.43845314\n",
      " 0.27189185], B intercept: 0.027898171149233545, Train loss: 0.06797, Test loss: 0.07788\n",
      "\n",
      "-- Epoch no(iteration no)  193\n",
      "W intercept: [0.68513412 0.54540392 0.58772404 0.48065094 0.45224629 0.4386369\n",
      " 0.27208932], B intercept: 0.027630787531969958, Train loss: 0.06788, Test loss: 0.07778\n",
      "\n",
      "-- Epoch no(iteration no)  194\n",
      "W intercept: [0.68614545 0.54631524 0.59107952 0.48223556 0.45150681 0.43827874\n",
      " 0.27341607], B intercept: 0.0317466944279156, Train loss: 0.06770, Test loss: 0.07766\n",
      "\n",
      "-- Epoch no(iteration no)  195\n",
      "W intercept: [0.6881321  0.54586737 0.59271507 0.48332904 0.45243824 0.43975566\n",
      " 0.27508989], B intercept: 0.032826366361787956, Train loss: 0.06754, Test loss: 0.07754\n",
      "\n",
      "-- Epoch no(iteration no)  196\n",
      "W intercept: [0.69131735 0.54623295 0.59404207 0.4825352  0.45357629 0.44055885\n",
      " 0.27552782], B intercept: 0.032492946580691504, Train loss: 0.06740, Test loss: 0.07741\n",
      "\n",
      "-- Epoch no(iteration no)  197\n",
      "W intercept: [0.69293937 0.54693159 0.59469037 0.48381988 0.45467581 0.44230895\n",
      " 0.27577166], B intercept: 0.032132664347951354, Train loss: 0.06727, Test loss: 0.07728\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch no(iteration no)  198\n",
      "W intercept: [0.69524275 0.54734831 0.5958056  0.48409    0.45519604 0.44227236\n",
      " 0.2760846 ], B intercept: 0.03256954094325694, Train loss: 0.06715, Test loss: 0.07718\n",
      "\n",
      "-- Epoch no(iteration no)  199\n",
      "W intercept: [0.69788746 0.54868163 0.59699242 0.48531767 0.45489524 0.44260376\n",
      " 0.27543017], B intercept: 0.034986335584006246, Train loss: 0.06699, Test loss: 0.07707\n",
      "\n",
      "-- Epoch no(iteration no)  200\n",
      "W intercept: [0.6996707  0.54911827 0.59877968 0.48619461 0.45753096 0.44357864\n",
      " 0.27532864], B intercept: 0.036493553749670886, Train loss: 0.06682, Test loss: 0.07694\n",
      "\n",
      "-- Epoch no(iteration no)  201\n",
      "W intercept: [0.70094952 0.54983553 0.60022184 0.48665069 0.4580986  0.44337019\n",
      " 0.27725664], B intercept: 0.035681390169533894, Train loss: 0.06671, Test loss: 0.07683\n",
      "\n",
      "-- Epoch no(iteration no)  202\n",
      "W intercept: [0.70269127 0.55047161 0.60289949 0.48758557 0.45807126 0.44432714\n",
      " 0.27780128], B intercept: 0.03545120134150272, Train loss: 0.06656, Test loss: 0.07667\n",
      "\n",
      "-- Epoch no(iteration no)  203\n",
      "W intercept: [0.70388582 0.5524304  0.60323357 0.48851533 0.45842511 0.4449419\n",
      " 0.27806216], B intercept: 0.03529212683542029, Train loss: 0.06646, Test loss: 0.07657\n",
      "\n",
      "-- Epoch no(iteration no)  204\n",
      "W intercept: [0.70576615 0.55398523 0.60407076 0.48961836 0.45964566 0.44618826\n",
      " 0.27857777], B intercept: 0.03451159853018124, Train loss: 0.06631, Test loss: 0.07642\n",
      "\n",
      "-- Epoch no(iteration no)  205\n",
      "W intercept: [0.70706685 0.55706495 0.60474138 0.49038253 0.46104479 0.44741632\n",
      " 0.27874974], B intercept: 0.03439110981106097, Train loss: 0.06615, Test loss: 0.07627\n",
      "\n",
      "-- Epoch no(iteration no)  206\n",
      "W intercept: [0.70872231 0.55913351 0.60538188 0.49181623 0.462464   0.44786222\n",
      " 0.27966909], B intercept: 0.033179418201239215, Train loss: 0.06601, Test loss: 0.07612\n",
      "\n",
      "-- Epoch no(iteration no)  207\n",
      "W intercept: [0.70780635 0.56113013 0.60715654 0.49343399 0.46296926 0.44722447\n",
      " 0.27943172], B intercept: 0.03360021868312709, Train loss: 0.06593, Test loss: 0.07603\n",
      "\n",
      "-- Epoch no(iteration no) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 238/300 [00:00<00:00, 262.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 208\n",
      "W intercept: [0.70875716 0.56130279 0.61077016 0.49484208 0.46203264 0.44778024\n",
      " 0.28144644], B intercept: 0.03708303346879711, Train loss: 0.06576, Test loss: 0.07592\n",
      "\n",
      "-- Epoch no(iteration no)  209\n",
      "W intercept: [0.71068005 0.5612901  0.61146758 0.49563549 0.46339727 0.44906173\n",
      " 0.28258678], B intercept: 0.03796413295232932, Train loss: 0.06564, Test loss: 0.07583\n",
      "\n",
      "-- Epoch no(iteration no)  210\n",
      "W intercept: [0.71377324 0.5614704  0.61261354 0.49487903 0.46444221 0.44986791\n",
      " 0.28290701], B intercept: 0.03768049057948473, Train loss: 0.06551, Test loss: 0.07572\n",
      "\n",
      "-- Epoch no(iteration no)  211\n",
      "W intercept: [0.71498899 0.56161956 0.61298718 0.49564241 0.46524872 0.4500938\n",
      " 0.28284452], B intercept: 0.03815242658788401, Train loss: 0.06544, Test loss: 0.07566\n",
      "\n",
      "-- Epoch no(iteration no)  212\n",
      "W intercept: [0.71763272 0.56242582 0.61441722 0.49652521 0.4656537  0.45124971\n",
      " 0.28334273], B intercept: 0.03812479191625348, Train loss: 0.06529, Test loss: 0.07552\n",
      "\n",
      "-- Epoch no(iteration no)  213\n",
      "W intercept: [0.71960007 0.56383418 0.61562951 0.49770429 0.46575372 0.45150527\n",
      " 0.28295235], B intercept: 0.04010929338637168, Train loss: 0.06515, Test loss: 0.07542\n",
      "\n",
      "-- Epoch no(iteration no)  214\n",
      "W intercept: [0.72149032 0.56396189 0.61743242 0.49785224 0.46776099 0.45121149\n",
      " 0.28408607], B intercept: 0.04173814502947632, Train loss: 0.06501, Test loss: 0.07532\n",
      "\n",
      "-- Epoch no(iteration no)  215\n",
      "W intercept: [0.72278461 0.56504539 0.61860788 0.49889004 0.46861719 0.45206328\n",
      " 0.28454164], B intercept: 0.040742355921272, Train loss: 0.06490, Test loss: 0.07520\n",
      "\n",
      "-- Epoch no(iteration no)  216\n",
      "W intercept: [0.72438688 0.56578869 0.62097039 0.49987376 0.46852658 0.45294639\n",
      " 0.28519989], B intercept: 0.04040997536104248, Train loss: 0.06477, Test loss: 0.07507\n",
      "\n",
      "-- Epoch no(iteration no)  217\n",
      "W intercept: [0.72572084 0.56783496 0.62130791 0.50051514 0.46890537 0.45348995\n",
      " 0.28533981], B intercept: 0.04032083748016541, Train loss: 0.06467, Test loss: 0.07497\n",
      "\n",
      "-- Epoch no(iteration no)  218\n",
      "W intercept: [0.72723705 0.56911476 0.62233914 0.5018868  0.47005986 0.45465563\n",
      " 0.28575077], B intercept: 0.039771515400678144, Train loss: 0.06454, Test loss: 0.07484\n",
      "\n",
      "-- Epoch no(iteration no)  219\n",
      "W intercept: [0.72860105 0.5721042  0.6225528  0.50254817 0.47136614 0.4560359\n",
      " 0.28618334], B intercept: 0.03925337537240087, Train loss: 0.06440, Test loss: 0.07471\n",
      "\n",
      "-- Epoch no(iteration no)  220\n",
      "W intercept: [0.73010349 0.57386716 0.62327959 0.50371706 0.47283052 0.45619457\n",
      " 0.28698131], B intercept: 0.0381888681688399, Train loss: 0.06428, Test loss: 0.07458\n",
      "\n",
      "-- Epoch no(iteration no)  221\n",
      "W intercept: [0.72925536 0.57607518 0.62484656 0.50543586 0.47320243 0.45571891\n",
      " 0.28651525], B intercept: 0.039188819823964355, Train loss: 0.06420, Test loss: 0.07450\n",
      "\n",
      "-- Epoch no(iteration no)  222\n",
      "W intercept: [0.73038128 0.57535267 0.62858212 0.5068499  0.47248322 0.45637823\n",
      " 0.28862776], B intercept: 0.04216909938591116, Train loss: 0.06405, Test loss: 0.07440\n",
      "\n",
      "-- Epoch no(iteration no)  223\n",
      "W intercept: [0.73223355 0.57571355 0.62929514 0.50742416 0.47356881 0.45761806\n",
      " 0.28978855], B intercept: 0.04315918314981917, Train loss: 0.06393, Test loss: 0.07432\n",
      "\n",
      "-- Epoch no(iteration no)  224\n",
      "W intercept: [0.73477562 0.57600975 0.63009073 0.50674098 0.47457368 0.45790439\n",
      " 0.28988552], B intercept: 0.04305203589052386, Train loss: 0.06384, Test loss: 0.07424\n",
      "\n",
      "-- Epoch no(iteration no)  225\n",
      "W intercept: [0.73635576 0.57620549 0.63041682 0.50732323 0.47527708 0.45803325\n",
      " 0.28989431], B intercept: 0.04335375602748991, Train loss: 0.06377, Test loss: 0.07418\n",
      "\n",
      "-- Epoch no(iteration no)  226\n",
      "W intercept: [0.73946774 0.57730333 0.63206631 0.507343   0.47474089 0.45869527\n",
      " 0.28984055], B intercept: 0.0447712788971199, Train loss: 0.06362, Test loss: 0.07407\n",
      "\n",
      "-- Epoch no(iteration no)  227\n",
      "W intercept: [0.74061707 0.57821925 0.6332117  0.5090536  0.47616973 0.45932335\n",
      " 0.29004448], B intercept: 0.045291657961597905, Train loss: 0.06350, Test loss: 0.07396\n",
      "\n",
      "-- Epoch no(iteration no)  228\n",
      "W intercept: [0.74214792 0.57830076 0.63487208 0.50930914 0.47764732 0.45901719\n",
      " 0.29113412], B intercept: 0.046688710005563135, Train loss: 0.06339, Test loss: 0.07389\n",
      "\n",
      "-- Epoch no(iteration no)  229\n",
      "W intercept: [0.74344533 0.57933741 0.63597639 0.51023163 0.4785113  0.45994014\n",
      " 0.29157523], B intercept: 0.04565541430270519, Train loss: 0.06329, Test loss: 0.07378\n",
      "\n",
      "-- Epoch no(iteration no)  230\n",
      "W intercept: [0.74501764 0.58012077 0.63812315 0.51125898 0.47848258 0.46081741\n",
      " 0.29219561], B intercept: 0.04544595686530052, Train loss: 0.06316, Test loss: 0.07365\n",
      "\n",
      "-- Epoch no(iteration no)  231\n",
      "W intercept: [0.74641136 0.58203165 0.63859635 0.51197453 0.47889858 0.46135049\n",
      " 0.29236887], B intercept: 0.04520099218568271, Train loss: 0.06307, Test loss: 0.07356\n",
      "\n",
      "-- Epoch no(iteration no)  232\n",
      "W intercept: [0.74776487 0.58458956 0.63921322 0.51276408 0.47975777 0.46246571\n",
      " 0.29251747], B intercept: 0.045273994615561135, Train loss: 0.06294, Test loss: 0.07344\n",
      "\n",
      "-- Epoch no(iteration no)  233\n",
      "W intercept: [0.74884897 0.58618201 0.63984199 0.51401104 0.48096747 0.4637129\n",
      " 0.29306719], B intercept: 0.044468468386993136, Train loss: 0.06284, Test loss: 0.07333\n",
      "\n",
      "-- Epoch no(iteration no)  234\n",
      "W intercept: [0.74901343 0.58842245 0.64028028 0.51525208 0.48189342 0.46255005\n",
      " 0.29409463], B intercept: 0.04291098914120121, Train loss: 0.06277, Test loss: 0.07324\n",
      "\n",
      "-- Epoch no(iteration no)  235\n",
      "W intercept: [0.74928976 0.58909878 0.64284295 0.51665127 0.4822133  0.46240408\n",
      " 0.29469451], B intercept: 0.045737432319379295, Train loss: 0.06265, Test loss: 0.07317\n",
      "\n",
      "-- Epoch no(iteration no)  236\n",
      "W intercept: [0.75061422 0.58915961 0.64568557 0.51796754 0.48205472 0.46384301\n",
      " 0.29553731], B intercept: 0.047139567999235736, Train loss: 0.06252, Test loss: 0.07306\n",
      "\n",
      "-- Epoch no(iteration no)  237\n",
      "W intercept: [0.75313811 0.5891664  0.64638662 0.5171049  0.48283124 0.46417746\n",
      " 0.29663862], B intercept: 0.0481430858930752, Train loss: 0.06242, Test loss: 0.07300\n",
      "\n",
      "-- Epoch no(iteration no)  238\n",
      "W intercept: [0.75503756 0.58977749 0.64691464 0.51792602 0.48405189 0.46541246\n",
      " 0.29681732], B intercept: 0.04783684948758959, Train loss: 0.06232, Test loss: 0.07291\n",
      "\n",
      "-- Epoch no(iteration no)  239\n",
      "W intercept: [0.75643867 0.58997962 0.64754367 0.51837265 0.48463959 0.46552586\n",
      " 0.29693244], B intercept: 0.048326531587463244, Train loss: 0.06225, Test loss: 0.07286\n",
      "\n",
      "-- Epoch no(iteration no)  240\n",
      "W intercept: [0.75949473 0.59091256 0.64876298 0.51873411 0.48419558 0.46586046\n",
      " 0.29643417], B intercept: 0.050342813943646814, Train loss: 0.06213, Test loss: 0.07277\n",
      "\n",
      "-- Epoch no(iteration no)  241\n",
      "W intercept: [0.76098146 0.59164779 0.65048943 0.52002152 0.4852695  0.46646503\n",
      " 0.29654162], B intercept: 0.051242540653051054, Train loss: 0.06200, Test loss: 0.07267\n",
      "\n",
      "-- Epoch no(iteration no)  242\n",
      "W intercept: [0.7619244  0.59186706 0.65155646 0.52012101 0.48691363 0.46617784\n",
      " 0.29790133], B intercept: 0.05162280895612395, Train loss: 0.06192, Test loss: 0.07261\n",
      "\n",
      "-- Epoch no(iteration no)  243\n",
      "W intercept: [0.76325275 0.59282765 0.65328523 0.52134281 0.48725376 0.46727571\n",
      " 0.29816409], B intercept: 0.05097376774980824, Train loss: 0.06182, Test loss: 0.07250\n",
      "\n",
      "-- Epoch no(iteration no)  244\n",
      "W intercept: [0.76453187 0.59353788 0.65446877 0.52126779 0.48818712 0.46749898\n",
      " 0.29826387], B intercept: 0.052082389746553534, Train loss: 0.06173, Test loss: 0.07244\n",
      "\n",
      "-- Epoch no(iteration no)  245\n",
      "W intercept: [0.76595909 0.59552681 0.655017   0.5227341  0.48813656 0.46842141\n",
      " 0.29905346], B intercept: 0.0503565992380297, Train loss: 0.06163, Test loss: 0.07232\n",
      "\n",
      "-- Epoch no(iteration no)  246\n",
      "W intercept: [0.7673074  0.5979911  0.65563465 0.52335427 0.48891264 0.46950317\n",
      " 0.29922103], B intercept: 0.050436855421258364, Train loss: 0.06152, Test loss: 0.07221\n",
      "\n",
      "-- Epoch no(iteration no)  247\n",
      "W intercept: [0.76865155 0.5995729  0.65633729 0.52406192 0.49032235 0.46961001\n",
      " 0.29991416], B intercept: 0.049259009508286954, Train loss: 0.06143, Test loss: 0.07211\n",
      "\n",
      "-- Epoch no(iteration no)  248\n",
      "W intercept: [0.76775069 0.60166416 0.65703099 0.52601726 0.49098685 0.46915412\n",
      " 0.30019389], B intercept: 0.04849842484058622, Train loss: 0.06138, Test loss: 0.07204\n",
      "\n",
      "-- Epoch no(iteration no)  249\n",
      "W intercept: [0.76825072 0.60240686 0.66038294 0.52672513 0.49014852 0.46914279\n",
      " 0.30133558], B intercept: 0.05254601003192778, Train loss: 0.06125, Test loss: 0.07198\n",
      "\n",
      "-- Epoch no(iteration no)  250\n",
      "W intercept: [0.76977387 0.60184207 0.6617036  0.52817638 0.49105629 0.47028008\n",
      " 0.30260768], B intercept: 0.053919149114597324, Train loss: 0.06114, Test loss: 0.07191\n",
      "\n",
      "-- Epoch no(iteration no)  251\n",
      "W intercept: [0.77264902 0.60209441 0.66272544 0.52725724 0.49196512 0.47074232\n",
      " 0.30298831], B intercept: 0.05372064408534804, Train loss: 0.06104, Test loss: 0.07182\n",
      "\n",
      "-- Epoch no(iteration no)  252\n",
      "W intercept: [0.77408322 0.60265321 0.66324139 0.52826191 0.49285222 0.47225668\n",
      " 0.30319999], B intercept: 0.05327729547844248, Train loss: 0.06096, Test loss: 0.07174\n",
      "\n",
      "-- Epoch no(iteration no)  253\n",
      "W intercept: [0.7760616  0.60281564 0.66406215 0.52842596 0.49330767 0.47199803\n",
      " 0.30337209], B intercept: 0.05367670645417282, Train loss: 0.06089, Test loss: 0.07169\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch no(iteration no)  254\n",
      "W intercept: [0.77840994 0.60392971 0.66497062 0.52926691 0.49280737 0.47227164\n",
      " 0.3027873 ], B intercept: 0.05587942071624124, Train loss: 0.06078, Test loss: 0.07162\n",
      "\n",
      "-- Epoch no(iteration no)  255\n",
      "W intercept: [0.78024913 0.60447932 0.66676767 0.53015604 0.49411998 0.47281569\n",
      " 0.30280504], B intercept: 0.05679272156719544, Train loss: 0.06066, Test loss: 0.07152\n",
      "\n",
      "-- Epoch no(iteration no)  256\n",
      "W intercept: [0.7807549  0.604772   0.6675534  0.53028425 0.49557948 0.47270582\n",
      " 0.3043154 ], B intercept: 0.05674614088338486, Train loss: 0.06060, Test loss: 0.07148\n",
      "\n",
      "-- Epoch no(iteration no)  257\n",
      "W intercept: [0.78225002 0.60517442 0.6700181  0.53110699 0.49547401 0.47350427\n",
      " 0.30484016], B intercept: 0.056545404776445346, Train loss: 0.06049, Test loss: 0.07137\n",
      "\n",
      "-- Epoch no(iteration no)  258\n",
      "W intercept: [0.78313152 0.60677389 0.67027306 0.53180811 0.49554042 0.47387982\n",
      " 0.30496029], B intercept: 0.056584921085424886, Train loss: 0.06043, Test loss: 0.07131\n",
      "\n",
      "-- Epoch no(iteration no)  259\n",
      "W intercept: [0.78472428 0.60809279 0.67092198 0.53272849 0.49657107 0.47495882\n",
      " 0.3054154 ], B intercept: 0.055829582576136794, Train loss: 0.06033, Test loss: 0.07121\n",
      "\n",
      "-- Epoch no(iteration no)  260\n",
      "W intercept: [0.78578107 0.61077257 0.67136793 0.53330591 0.49770272 0.4759247\n",
      " 0.30551145], B intercept: 0.05584408004739213, Train loss: 0.06023, Test loss: 0.07112\n",
      "\n",
      "-- Epoch no(iteration no)  261\n",
      "W intercept: [0.7871359  0.61247721 0.67192364 0.53442442 0.49888661 0.47618487\n",
      " 0.3062906 ], B intercept: 0.05479713602405985, Train loss: 0.06014, Test loss: 0.07102\n",
      "\n",
      "-- Epoch no(iteration no)  262\n",
      "W intercept: [0.78609006 0.61430019 0.67343777 0.53595663 0.49927868 0.47531507\n",
      " 0.30602097], B intercept: 0.05510709661185701, Train loss: 0.06009, Test loss: 0.07097\n",
      "\n",
      "-- Epoch no(iteration no)  263\n",
      "W intercept: [0.78684862 0.61455405 0.67621958 0.53727189 0.49861845 0.47577212\n",
      " 0.30748906], B intercept: 0.05809761471640065, Train loss: 0.05997, Test loss: 0.07090\n",
      "\n",
      "-- Epoch no(iteration no)  264\n",
      "W intercept: [0.78845601 0.61428748 0.67724051 0.53783307 0.49952735 0.47670587\n",
      " 0.30887263], B intercept: 0.05934140512381367, Train loss: 0.05988, Test loss: 0.07084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:01<00:00, 259.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Epoch no(iteration no)  265\n",
      "W intercept: [0.79128238 0.61433725 0.67817984 0.53690495 0.50037412 0.47729242\n",
      " 0.30913911], B intercept: 0.05914289059914941, Train loss: 0.05979, Test loss: 0.07077\n",
      "\n",
      "-- Epoch no(iteration no)  266\n",
      "W intercept: [0.79231414 0.61432894 0.67842693 0.5373983  0.50101033 0.47733229\n",
      " 0.30904552], B intercept: 0.05959912069285903, Train loss: 0.05975, Test loss: 0.07074\n",
      "\n",
      "-- Epoch no(iteration no)  267\n",
      "W intercept: [0.79463009 0.61496586 0.67954939 0.53812859 0.50131323 0.47829478\n",
      " 0.30946649], B intercept: 0.05947238094740002, Train loss: 0.05965, Test loss: 0.07065\n",
      "\n",
      "-- Epoch no(iteration no)  268\n",
      "W intercept: [0.79644422 0.6162467  0.68048036 0.53904341 0.50105565 0.47823141\n",
      " 0.30904876], B intercept: 0.06127232006999591, Train loss: 0.05955, Test loss: 0.07059\n",
      "\n",
      "-- Epoch no(iteration no)  269\n",
      "W intercept: [0.79778629 0.61619544 0.68210173 0.53914582 0.50294395 0.47803787\n",
      " 0.30995901], B intercept: 0.06278357632118733, Train loss: 0.05946, Test loss: 0.07054\n",
      "\n",
      "-- Epoch no(iteration no)  270\n",
      "W intercept: [0.79888057 0.61705231 0.68293826 0.53992103 0.5037243  0.47874969\n",
      " 0.31042907], B intercept: 0.06203858774936481, Train loss: 0.05939, Test loss: 0.07046\n",
      "\n",
      "-- Epoch no(iteration no)  271\n",
      "W intercept: [0.80024362 0.61755169 0.68508584 0.54073296 0.50354653 0.47945921\n",
      " 0.31100815], B intercept: 0.06183323122523148, Train loss: 0.05929, Test loss: 0.07037\n",
      "\n",
      "-- Epoch no(iteration no)  272\n",
      "W intercept: [0.80127979 0.61926962 0.68536438 0.54122961 0.50366918 0.47983875\n",
      " 0.31105292], B intercept: 0.06189776863855905, Train loss: 0.05923, Test loss: 0.07031\n",
      "\n",
      "-- Epoch no(iteration no)  273\n",
      "W intercept: [0.80253438 0.62035988 0.68621588 0.5423408  0.50464215 0.48081801\n",
      " 0.31144923], B intercept: 0.061299841646103324, Train loss: 0.05914, Test loss: 0.07022\n",
      "\n",
      "-- Epoch no(iteration no)  274\n",
      "W intercept: [0.80366073 0.62298315 0.68632552 0.54289488 0.50571905 0.48195967\n",
      " 0.31174554], B intercept: 0.0609928343815552, Train loss: 0.05905, Test loss: 0.07013\n",
      "\n",
      "-- Epoch no(iteration no)  275\n",
      "W intercept: [0.80490441 0.62450194 0.68692509 0.54381141 0.50692539 0.48190471\n",
      " 0.31241   ], B intercept: 0.06010126453084884, Train loss: 0.05897, Test loss: 0.07005\n",
      "\n",
      "-- Epoch no(iteration no)  276\n",
      "W intercept: [0.80399781 0.62646956 0.68825789 0.54537881 0.50723452 0.48126206\n",
      " 0.31201305], B intercept: 0.06088060173704093, Train loss: 0.05892, Test loss: 0.07000\n",
      "\n",
      "-- Epoch no(iteration no)  277\n",
      "W intercept: [0.80481503 0.62577489 0.6917501  0.54656571 0.50646397 0.48169748\n",
      " 0.31393335], B intercept: 0.06388934056997783, Train loss: 0.05881, Test loss: 0.06994\n",
      "\n",
      "-- Epoch no(iteration no)  278\n",
      "W intercept: [0.80636948 0.62597803 0.6922492  0.54711893 0.50732525 0.48265859\n",
      " 0.31488312], B intercept: 0.06494050778610555, Train loss: 0.05873, Test loss: 0.06989\n",
      "\n",
      "-- Epoch no(iteration no)  279\n",
      "W intercept: [0.80882502 0.62620554 0.69290465 0.54628977 0.50828714 0.4828767\n",
      " 0.31502375], B intercept: 0.0646669785297735, Train loss: 0.05866, Test loss: 0.06983\n",
      "\n",
      "-- Epoch no(iteration no)  280\n",
      "W intercept: [0.81012214 0.62623236 0.69289973 0.54672688 0.50881267 0.48280799\n",
      " 0.31503897], B intercept: 0.06487979658824507, Train loss: 0.05862, Test loss: 0.06981\n",
      "\n",
      "-- Epoch no(iteration no)  281\n",
      "W intercept: [0.81307333 0.62741938 0.69401477 0.54669678 0.50872373 0.48352025\n",
      " 0.31511412], B intercept: 0.06569293895564922, Train loss: 0.05851, Test loss: 0.06972\n",
      "\n",
      "-- Epoch no(iteration no)  282\n",
      "W intercept: [0.81388705 0.62789296 0.69533411 0.54805709 0.50938749 0.4838319\n",
      " 0.31495764], B intercept: 0.06683696366112316, Train loss: 0.05843, Test loss: 0.06967\n",
      "\n",
      "-- Epoch no(iteration no)  283\n",
      "W intercept: [0.81512212 0.62788552 0.69672906 0.54816395 0.51077831 0.48349255\n",
      " 0.31600969], B intercept: 0.06808526729351037, Train loss: 0.05835, Test loss: 0.06962\n",
      "\n",
      "-- Epoch no(iteration no)  284\n",
      "W intercept: [0.81618331 0.62872821 0.69758963 0.54892911 0.51148403 0.48423684\n",
      " 0.316377  ], B intercept: 0.06721482653078724, Train loss: 0.05828, Test loss: 0.06955\n",
      "\n",
      "-- Epoch no(iteration no)  285\n",
      "W intercept: [0.81755754 0.62930062 0.69961617 0.54980866 0.51138116 0.48497697\n",
      " 0.3169297 ], B intercept: 0.06706011699811888, Train loss: 0.05819, Test loss: 0.06946\n",
      "\n",
      "-- Epoch no(iteration no)  286\n",
      "W intercept: [0.81863478 0.63090443 0.69999415 0.55032937 0.51151126 0.48529134\n",
      " 0.31699845], B intercept: 0.06705135775714764, Train loss: 0.05813, Test loss: 0.06940\n",
      "\n",
      "-- Epoch no(iteration no)  287\n",
      "W intercept: [0.81973472 0.63320624 0.70047509 0.5509513  0.51224549 0.48625306\n",
      " 0.31711463], B intercept: 0.0671272170890363, Train loss: 0.05805, Test loss: 0.06933\n",
      "\n",
      "-- Epoch no(iteration no)  288\n",
      "W intercept: [0.8206929  0.63457021 0.70092994 0.55205671 0.51329717 0.48734282\n",
      " 0.31759402], B intercept: 0.06637173119777115, Train loss: 0.05798, Test loss: 0.06925\n",
      "\n",
      "-- Epoch no(iteration no)  289\n",
      "W intercept: [0.82068136 0.63657227 0.70129608 0.55313058 0.51404828 0.4860199\n",
      " 0.31855897], B intercept: 0.0648131129231783, Train loss: 0.05793, Test loss: 0.06919\n",
      "\n",
      "-- Epoch no(iteration no)  290\n",
      "W intercept: [0.82077861 0.63711882 0.70348594 0.55418375 0.51433027 0.48578078\n",
      " 0.31905819], B intercept: 0.0675057305990235, Train loss: 0.05785, Test loss: 0.06915\n",
      "\n",
      "-- Epoch no(iteration no)  291\n",
      "W intercept: [0.82190938 0.63715473 0.70626203 0.55549903 0.51407967 0.48697268\n",
      " 0.31983047], B intercept: 0.06898273888629382, Train loss: 0.05774, Test loss: 0.06907\n",
      "\n",
      "-- Epoch no(iteration no)  292\n",
      "W intercept: [0.82420102 0.63704528 0.70671049 0.55457995 0.51471589 0.48706155\n",
      " 0.32077777], B intercept: 0.06996359317376355, Train loss: 0.05768, Test loss: 0.06904\n",
      "\n",
      "-- Epoch no(iteration no)  293\n",
      "W intercept: [0.82591024 0.6375458  0.70717959 0.55525355 0.51578653 0.48816989\n",
      " 0.32092083], B intercept: 0.06967086768639685, Train loss: 0.05761, Test loss: 0.06897\n",
      "\n",
      "-- Epoch no(iteration no)  294\n",
      "W intercept: [0.82711862 0.6375807  0.70751518 0.55548376 0.51622949 0.48810534\n",
      " 0.32098075], B intercept: 0.07016645785255318, Train loss: 0.05757, Test loss: 0.06895\n",
      "\n",
      "-- Epoch no(iteration no)  295\n",
      "W intercept: [0.82986871 0.63833799 0.70855506 0.55539181 0.51588769 0.48833846\n",
      " 0.32055738], B intercept: 0.0718155536556453, Train loss: 0.05748, Test loss: 0.06889\n",
      "\n",
      "-- Epoch no(iteration no)  296\n",
      "W intercept: [0.83094692 0.63906457 0.7099323  0.55685335 0.51666672 0.48891166\n",
      " 0.32047123], B intercept: 0.07265186974920118, Train loss: 0.05739, Test loss: 0.06883\n",
      "\n",
      "-- Epoch no(iteration no)  297\n",
      "W intercept: [0.83180356 0.63907184 0.71090351 0.55683414 0.51811164 0.48848371\n",
      " 0.32177716], B intercept: 0.07325816010549413, Train loss: 0.05733, Test loss: 0.06879\n",
      "\n",
      "-- Epoch no(iteration no)  298\n",
      "W intercept: [0.83289514 0.63981117 0.71244952 0.55788586 0.51835714 0.48941828\n",
      " 0.32199259], B intercept: 0.07272597256912558, Train loss: 0.05726, Test loss: 0.06871\n",
      "\n",
      "-- Epoch no(iteration no)  299\n",
      "W intercept: [0.83400189 0.6403656  0.71353961 0.5577276  0.51915683 0.48953026\n",
      " 0.32204482], B intercept: 0.07382035872910782, Train loss: 0.05719, Test loss: 0.06868\n",
      "\n",
      "-- Epoch no(iteration no)  300\n",
      "W intercept: [0.83516376 0.64208605 0.71398808 0.55899002 0.51887601 0.49026535\n",
      " 0.32272818], B intercept: 0.0723450835283289, Train loss: 0.05713, Test loss: 0.06859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## importing self made SGD classifier as Classifier 1 \n",
    "import Sgd_classifier as classfier_1\n",
    "alpha=0.001\n",
    "t_rate=0.001\n",
    "N=len(x_train)\n",
    "epochs=300\n",
    "w,b,loss_train,loss_test=classfier_1.train_classifier(x_train,y_train,x_test,y_test,epochs,alpha,t_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNjgLcYRYcIm"
   },
   "source": [
    "<h4>2.1.2 Coding Approach</h4>\n",
    "We send our train data, test data, with learning rate and regularization rate to our classifier. The classifier uses Stochastic gradient descent and updates the weights and bais over the epochs. With each epoch we aim to reduce our loss, when we reach a point where the loss is similar to the loss that was in previous epoch we return then updated weights and bais. These weights and bais that are returned are optimal for making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndwbS6ZqaLr3"
   },
   "source": [
    "<h3>2.2 SVM with RBF kernel </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-Qtsx6LaTN7"
   },
   "source": [
    "<h4>2.2.1 Function Call</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ed3ef24f128d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# a1.fit(x_train,y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mSvm_classifier\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mclassfier_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassfier_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOurSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Project_Folder/MLIS_Project_Ideal/Svm_classifier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#     return np.exp(-(xx + zz.T - 2 * np.dot(x, z.T)) / (2 * sigma ** 2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mOurSVM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Project_Folder/MLIS_Project_Ideal/Svm_classifier.py\u001b[0m in \u001b[0;36mOurSVM\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0malphaX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m           \u001b[0malphaY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "# import Svm_classifier as classfier_2\n",
    "\n",
    "# a1=classfier_2.SVM(kernel='rbf', C=0.5, max_iter=10, gamma=0.001)\n",
    "# a1.fit(x_train,y_train)\n",
    "\n",
    "import Svm_classifier as classfier_2\n",
    "\n",
    "a1=classfier_2.OurSVM(kernel='linear', C=1.0, gamma=0.001)\n",
    "a1.fit(x_train, y_train)\n",
    "#y_ = a1.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUu-87k2aXIR"
   },
   "source": [
    "<h4>2.2.2 Coding Approach</h4>\n",
    "\n",
    "<!----> Will be going over the coding how it is done and stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swzuVJ19S1Jr"
   },
   "source": [
    "<h2><center>3. Model Accuracy</center></h2>\n",
    "<em>\n",
    "Here we are going to talk about the model accuracies by using test and train loss graphs, in addition to Confussion Matrix and some ROC curves\n",
    "</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPy611dMa8Bu"
   },
   "source": [
    "<h3>3.1 Model 1 SGD Classifier with Log Loss</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rI7qXtftah1a"
   },
   "source": [
    "<h4>3.1.1 Prediction accuracies</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "057fGPgc6x3_",
    "outputId": "c841cc8b-3706-45a5-886f-2b89fe684824",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting the predict vector for the train and test data\n",
    "y_train_pred = classfier_1.predict(w,b,x_train)\n",
    "y_test_pred = classfier_1.predict(w,b,x_test)\n",
    "\n",
    "#y_train=y_train.reshape(y_train_pred.shape)\n",
    "print('Train_Accuracy : {:.3f}'.format((y_train ==  y_train_pred).sum() / len(x_train)))\n",
    "#y_test=y_test.reshape(y_test_pred.shape)\n",
    "print('Test_Accuracy  : {:.3f}'.format((y_test ==  y_test_pred).sum() / len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pAXgaM_bjzs"
   },
   "source": [
    "<h4>3.1.2 Test and Train Loss over Epochs</h4>\n",
    "We will plot graph against the number of epochs with respect to the test and train loss. This will give us insight whether our code is performing gradient descent in a correct manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gK7yssz073T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting epochs to an array and ploting the graphs of test and train loss over the epochs\n",
    "epochs = np.arange(1, epochs+1, 1)\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(epochs,loss_train, label='Train Loss', c = 'blue')\n",
    "plt.plot(epochs,loss_test, label='Test Loss',c = 'red')\n",
    "plt.title('Epoch vs Train,Test Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "print(110*'=')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcN8YgFwb0gH"
   },
   "source": [
    "<h4>3.1.3 Confusion Matrix and Performance Metrics</h4>\n",
    "The confusion matrix we plot here looks like the table mentioned below :-\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><em><h4>Total Population</h4></em></th>\n",
    "    <th><em><h4>Malignant</h4></em></th>\n",
    "    <th><em><h4>Benign</h4></em></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center><em><h4>Predicted Malignant</h4></em></center></td>\n",
    "      <td><center><h5>True Positives</h5></center></td>\n",
    "    <td><center><h5>False Positives</h5></center></td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><em><h4>Predicted Benign</h4></em></td>\n",
    "     <td><center><h5>False Negatives</h5></center></td>\n",
    "     <td><center><h5>True Negatives</h5></center></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwjOiXQsTjjO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calling our confusion matrix from the directory\n",
    "\n",
    "import confusion_matrix as cm\n",
    "cm_sgd,fpr_sgd,recall_sgd,AUC_score_sgd,precision_sgd,F1_score_sgd= cm.confusion_matrix(y_test, y_test_pred)\n",
    "print('Confusion Matrix\\n',cm_sgd)\n",
    "print('False Positive Rate :   {:.3f}'.format(fpr_sgd))\n",
    "print('True Positive Rate  :   {:.3f}'.format(recall_sgd))\n",
    "print('Precision of model  :   {:.3f}'.format(precision_sgd))\n",
    "print('Area Under Curve (AUC): {:.3f}'.format(AUC_score_sgd))\n",
    "print('F1 score of model   :   {:.3f}'.format(F1_score_sgd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2 Model 2 SVM with RBF Kernel</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.2.1 Prediction accuracies</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predict vector for the train and test data\n",
    "\n",
    "y_train_pred_svm = a1.predict(x_train)\n",
    "y_test_pred_svm= a1.predict(x_test)\n",
    "print('Train_Accuracy : {:.3f}'.format((y_train ==  y_train_pred_svm).sum() / len(x_train)))\n",
    "print('Test_Accuracy  : {:.3f}'.format((y_test ==  y_test_pred_svm).sum() / len(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.2.2  APLS GRAPH</h4>\n",
    "<!---> Some grpah here ALP  <--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3.2.3 Confusion Matrix and Performance Metrics</h4> \n",
    "The confusion matrix we plot here looks like the table mentioned below :-\n",
    "<table>\n",
    "  <tr>\n",
    "    <th><em><h4>Total Population</h4></em></th>\n",
    "    <th><em><h4>Malignant</h4></em></th>\n",
    "    <th><em><h4>Benign</h4></em></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><center><em><h4>Predicted Malignant</h4></em></center></td>\n",
    "      <td><center><h5>True Positives</h5></center></td>\n",
    "    <td><center><h5>False Positives</h5></center></td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><em><h4>Predicted Benign</h4></em></td>\n",
    "     <td><center><h5>False Negatives</h5></center></td>\n",
    "     <td><center><h5>True Negatives</h5></center></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calling our confusion matrix from the directory\n",
    "\n",
    "import confusion_matrix as cm\n",
    "cm_svm,fpr_svm,recall_svm,AUC_score_svm,precision_svm,F1_score_svm= cm.confusion_matrix(y_test, y_test_pred_svm)\n",
    "print('Confusion Matrix\\n',cm_svm)\n",
    "print('False Positive Rate :   {:.3f}'.format(fpr_svm))\n",
    "print('True Positive Rate  :   {:.3f}'.format(recall_svm))\n",
    "print('Precision of model  :   {:.3f}'.format(precision_svm))\n",
    "print('Area Under Curve (AUC): {:.3f}'.format(AUC_score_svm))\n",
    "print('F1 score of model   :   {:.3f}'.format(F1_score_svm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Model Comparisions</center></h2>\n",
    "\n",
    "<h3><center>Metrics Comparsions</center></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a data frame of all the result \n",
    "cols = ['Metric' , 'SGD Classifier' , 'SVM Classifier']\n",
    "values = [['F1_score',F1_score_sgd,F1_score_svm] ,[ 'Recal',recall_sgd,recall_svm]\\\n",
    "          ,['FPR',fpr_sgd,fpr_svm],['AUC',AUC_score_sgd,AUC_score_svm]\\\n",
    "          ,['Precision',precision_sgd,precision_svm]]\n",
    "result = pd.DataFrame(values, columns = cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW2Czt08hhqg"
   },
   "source": [
    "<h2><center>Conclusions</center></h2>\n",
    "We were given a task to implement a model which was meant to classify between Malignant and Benign and measure the performance metrics of the implemented model. To achive the aim of the coursework we made two classification models that perform binary classification for the given dataset. \n",
    "<br> </br>\n",
    "<p>\n",
    "<em>\n",
    "The following are the results obtained for the repesctive implemented models :-\n",
    "<p></em>\n",
    "<em>    \n",
    "    \n",
    "<br><h4>1. Stochastic Gradient Descent Classifier </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing only the SGD classifier from Result\n",
    "result.loc[:,['Metric','SGD Classifier']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<em><br><h4>2. Support Vector Machine </h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing only SVM classifier from Result\n",
    "result.loc[:,['Metric','SVM Classifier']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tB9iqYfg-NA"
   },
   "source": [
    "<h2><center> Coding References</center></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. [Confusion Matrix](https://www.nbshare.io/notebook/626706996/Learn-And-Code-Confusion-Matrix-With-Python/)\n",
    "2. [SGD Classifier <sup>[1]</sup>](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/logistic-regression.pdf)\n",
    "3. [SGD Classifier <sup>[2]</sup>]()\n",
    "4. [Pandas <sup>[1]</sup>](https://stackoverflow.com/questions/43772362/how-to-print-a-specific-row-of-a-pandas-dataframe)\n",
    "5. [Pandas <sup>[2]</sup>]()\n",
    "6. [SVM <sup>[1]</sup>]()\n",
    "7. [SVM <sup>[2]</sup>]()\n",
    "8. [SVM <sup>[3]</sup>]()\n",
    "9. [Numpy<sup>[1]</sup>](https://stackoverflow.com/questions/35932223/writing-a-train-test-split-function-with-numpy)\n",
    "\n",
    "\n",
    "<!--> ALP add some references add hyper links in there <--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Main_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
